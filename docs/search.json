[
  {
    "objectID": "Vignettes/GettingStarted.html",
    "href": "Vignettes/GettingStarted.html",
    "title": "Getting Started with DeToX",
    "section": "",
    "text": "Great! You‚Äôve got DeToX installed‚Äînow let‚Äôs jump into the exciting part!\nThis tutorial will walk you through an EXTREMELY basic example showing what DeToX can do and what you‚Äôll need to get started. Think of it as your quick-start guide to running your first eye-tracking experiment.\nDeToX bridges two powerful Python libraries: PsychoPy and tobii_research.\nThat‚Äôs where DeToX comes in: we‚Äôve wrapped the tricky bits so you can focus on your research, not wrestling with SDK documentation.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "Vignettes/GettingStarted.html#preparation",
    "href": "Vignettes/GettingStarted.html#preparation",
    "title": "Getting Started with DeToX",
    "section": "Preparation",
    "text": "Preparation\nlet‚Äôs begin importing the libraries that we will need for this example\n\nfrom psychopy import visual, core\nfrom DeToX import ETracker\n\nvisual and core are some of PsychoPy‚Äôs main modules‚Äîit‚Äôs what you‚Äôll use to create the window where your stimuli appear and your experiment runs.\nETracker is DeToX‚Äôs main class and your central hub for all eye-tracking operations. This is the object you‚Äôll interact with throughout your experiment to control calibration, recording, and data collection.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "Vignettes/GettingStarted.html#window",
    "href": "Vignettes/GettingStarted.html#window",
    "title": "Getting Started with DeToX",
    "section": "Window",
    "text": "Window\nEvery experiment needs a stage‚Äîin PsychoPy, that‚Äôs your Window. This is where all your stimuli will appear and where participants will interact with your study.\n\n# Create the experiment window\nwin = visual.Window(\n    size=[1920, 1080],  # Window dimensions in pixels\n    fullscr=True,       # Expand to fill the entire screen\n    units='pix'         # Use pixels as the measurement unit\n)\n\nBreaking it down:\n\nsize: Sets your window dimensions. Here we‚Äôre using 1920√ó1080, but adjust this to match your monitor.\nfullscr=True: Makes the window take over the whole screen‚Äîcrucial for experiments where you want to eliminate distractions.\nunits='pix': Defines how you‚Äôll specify positions and sizes throughout your experiment. DeToX supports multiple PsychoPy unit systems‚Äî'height', 'norm', 'pix'‚Äîso choose whichever you‚Äôre most comfortable with or best fits your experimental design.\n\n\n\n\n\n\n\nWindow size\n\n\n\nIf you‚Äôre following along with this tutorial and experimenting on your own, we strongly recommend using a smaller window with fullscr=False instead of fullscreen mode. When fullscr=True, the window takes over your entire screen, making it tricky (or impossible!) to interact with your computer‚Äîlike stopping the script or checking documentation. Save fullscreen for your actual experiments.\n\n\nPerfect now we have our window where we can draw images, videos and interact with them!!",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "Vignettes/GettingStarted.html#etracker",
    "href": "Vignettes/GettingStarted.html#etracker",
    "title": "Getting Started with DeToX",
    "section": "ETracker",
    "text": "ETracker\nSo far we‚Äôve focused on creating the canvas for our stimuli‚Äîbut how do we actually interact with the eye tracker? Simple! We use the ETracker class we imported earlier.\nThe ETracker needs access to the window we just created, so initializing it is straightforward:\n\nET_controller = ETracker(win)\n\n\n\n\n\n\n\nDon‚Äôt Have an Eye Tracker? No Problem!\n\n\n\nIf you‚Äôre following along without a Tobii eye tracker connected, you can still test everything using simulation mode. Just pass simulate=True when creating your ETracker:\n\nET_controller = ETracker(win, simulate=True)\n\nThis tells DeToX to collect data from your mouse position instead of an actual eye tracker‚Äîperfect for development, testing, or learning the workflow before you have hardware access üòâ\n\n\nOnce you run this code, DeToX will connect to your eye tracker and set everything up for you. It will also gather information about the connected device and display it in a nice, readable format:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Eyetracker Info ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇConnected to the eyetracker:                         ‚îÇ\n‚îÇ - Model: Tobii Pro Fusion                           ‚îÇ\n‚îÇ - Current frequency: 250.0 Hz                       ‚îÇ\n‚îÇ - Current illumination mode: Default                ‚îÇ\n‚îÇOther options:                                       ‚îÇ\n‚îÇ - Possible frequencies: (30.0, 60.0, 120.0, 250.0)  ‚îÇ\n‚îÇ - Possible illumination modes: ('Default',)         ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nThis tells us we‚Äôre connected to the eye tracker and ready to start recording data!",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "Vignettes/GettingStarted.html#recod-data",
    "href": "Vignettes/GettingStarted.html#recod-data",
    "title": "Getting Started with DeToX",
    "section": "Recod data",
    "text": "Recod data\nGreat! You‚Äôre now connected to the eye-tracker (or simulating it). However, we‚Äôre not actually collecting any data yet - let‚Äôs fix that.\nTo begin data collection, call the start_recording method on your ETracker instance:\n\n# Start recording data\nET_controller.start_recording(filename=\"testing.h5\")\n\nThe start_recording method accepts a filename parameter for naming your data file. If you don‚Äôt specify one, DeToX automatically generates a timestamp-based filename.\nYour eye-tracking data is now being collected continuously and will be later saved in a HDF5 format, which is ideal for storing large datasets efficiently. For details on the data structure and how to analyze your files, see our DataFormats guide.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "Vignettes/GettingStarted.html#events",
    "href": "Vignettes/GettingStarted.html#events",
    "title": "Getting Started with DeToX",
    "section": "Events",
    "text": "Events\nOK, now that we‚Äôre recording data, we can show images, videos, or whatever we want! It‚Äôs entirely up to you and your experimental design!\nSince this is a SUPER BASIC example to get you started, we won‚Äôt overcomplicate things with elaborate stimuli or complex tasks. Let‚Äôs keep it stupidly simple. As we show images, videos or whatnot we need to keep track at which point thesee stimuli happen in our eyetracking data. And how to do so?? well we can use the record_event function!!\n\n# Send event 1\nET_controller.record_event('wait 1')\ncore.wait(2) # wait 2s\n\n# Send event 2\nET_controller.record_event('wait 2')\ncore.wait(2) # wait 2s\n\nHere‚Äôs what‚Äôs happening:\n\ncontroller.record_event('wait 1'): Drops a timestamped marker labeled 'wait 1' into your data stream. This is like planting a flag that says ‚Äúsomething important happened HERE.‚Äù\ncore.wait(2): Pauses execution for 2 seconds. During this time, the eye tracker keeps collecting gaze data in the background.\ncontroller.record_event('wait 2'): Plants another marker at the 2-second point, labeled 'wait 2'.\nAnother core.wait(2): Waits another 2 seconds.\n\nHere we‚Äôre just using core.wait() as a placeholder. In your actual experiment, this is where you‚Äôd display your stimuli‚Äîshow images, play videos, present text, or run whatever task your study requires. The record_event() calls mark when those stimuli begin in this case!",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "Vignettes/GettingStarted.html#stop-recording",
    "href": "Vignettes/GettingStarted.html#stop-recording",
    "title": "Getting Started with DeToX",
    "section": "Stop recording",
    "text": "Stop recording\nAfter the experiment is done, we need to stop the recording and save the data!!!\n\n# Stop recording data\nET_controller.stop_recording()\n\nVoil√†! DeToX will stop the recording and automatically save all your data to a file. You‚Äôll get another nice confirmation message showing you what happened:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Recording Complete ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇData collection lasted approximately 4.02 seconds‚îÇ\n‚îÇData has been saved to testing.h5                ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nThis tells you how long the recording session lasted and where your data file was saved. By default, DeToX creates a timestamped filename (like testing.h5) so you never accidentally overwrite previous recordings.\nAnd that‚Äôs it! Your eye-tracking data‚Äîcomplete with all those event markers you recorded‚Äîis now safely stored and ready for analysis.",
    "crumbs": [
      "Getting Started"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DeToX",
    "section": "",
    "text": "This package was created out of the need to run eye-tracking experiments using tobii_research. The Tobii SDK provides a powerful way to interact with Tobii eye trackers in Python, offering many advanced features. However, certain aspects of its implementation can be complex and unintuitive for routine research tasks. Since we often run experiments using PsychoPy, we developed this lightweight wrapper around PsychoPy and tobii_research to simplify the process. The goal is to make it easy to run infant-friendly eye-tracking studies while handling the more technical aspects of eye tracker integration behind the scenes.\nThe beauty of DeToX lies in its simplicity - if you already have PsychoPy installed, you only need to add DeToX and you‚Äôre ready to go. No additional dependencies, no complex setup procedures. It‚Äôs just a clean, intuitive wrapper that makes eye-tracking accessible.\nThis project didn‚Äôt start from scratch‚Äîit builds upon an existing repository that we have used in the past: psychopy_tobii_infant\nWhile this repository provided solid solutions for integrating Tobii eye trackers with PsychoPy, we added features and improvements that we found useful for running infant-friendly eye-tracking studies. Our goal was to simplify some of the more technical aspects while keeping the flexibility needed for research."
  },
  {
    "objectID": "index.html#simplicity",
    "href": "index.html#simplicity",
    "title": "DeToX",
    "section": "Simplicity",
    "text": "Simplicity\nWhile the eye-tracking landscape offers many exc llent tools‚Äîfrom PsychoPy‚Äôs built-in Tobii integration to comprehensive packages like Titta‚ÄîDeToX carves out its own niche through thoughtful simplicity. We‚Äôve prioritized clarity and ease-of-use without sacrificing the flexibility researchers need. When your codebase is straightforward and well-documented, it becomes a platform for innovation rather than an obstacle to overcome."
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "DeToX",
    "section": "Documentation",
    "text": "Documentation\nWe believe that good software is only as valuable as its documentation. Too often, researchers encounter powerful packages that become frustrating to use due to unclear or incomplete documentation. When you‚Äôre left wondering what a function does or how to modify basic settings, the package becomes more of a hindrance than a help. A package should empower users, not confuse them. That‚Äôs why we‚Äôve prioritized creating clear, comprehensive documentation with practical examples and step-by-step tutorials. Our goal is documentation that helps you understand not just what to do, but why‚Äîso you can confidently adapt the code to your specific experimental needs.\n\nVignettes\nOur vignettes offer hands-on explanations of DeToX‚Äôs settings and features through practical examples. Each vignette focuses on specific functionality, walking you through the implementation details and configuration options you‚Äôll need to effectively integrate these features into your research workflow.\n\n\nTutorial\nWhile vignettes dive deep into individual features, our tutorial take a different approach - it demonstrate a complete working experiment from start to finish. Rather than explaining every parameter in detail it shows you how all the pieces fit together in real research scenarios."
  },
  {
    "objectID": "api/tobii2pix.html",
    "href": "api/tobii2pix.html",
    "title": "",
    "section": "",
    "text": "Coordinate Conversiontobii2pix",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "tobii2pix"
    ]
  },
  {
    "objectID": "api/tobii2pix.html#parameters",
    "href": "api/tobii2pix.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwin\npsychopy.visual.Window\nThe PsychoPy window which provides screen dimensions for scaling.\nrequired\n\n\np\ntuple\nThe Tobii ADCS coordinates to convert as (x, y) in range [0, 1]. Origin is top-left, x increases rightward, y increases downward.\nrequired",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "tobii2pix"
    ]
  },
  {
    "objectID": "api/tobii2pix.html#returns",
    "href": "api/tobii2pix.html#returns",
    "title": "",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nThe converted PsychoPy pixel coordinates as (x, y) with origin at screen center. Values are rounded to nearest integer for pixel alignment.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "tobii2pix"
    ]
  },
  {
    "objectID": "api/tobii2pix.html#notes",
    "href": "api/tobii2pix.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\nThe conversion involves: 1. Shifting origin from top-left to center (-0.5 offset) 2. Scaling by screen dimensions to get pixel values 3. Inverting the Y-axis to match PsychoPy‚Äôs bottom-up convention\nOutput coordinates follow PsychoPy‚Äôs pixel convention where (0, 0) is at screen center.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "tobii2pix"
    ]
  },
  {
    "objectID": "api/pix2tobii.html",
    "href": "api/pix2tobii.html",
    "title": "",
    "section": "",
    "text": "Coordinate Conversionpix2tobii",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "pix2tobii"
    ]
  },
  {
    "objectID": "api/pix2tobii.html#parameters",
    "href": "api/pix2tobii.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwin\npsychopy.visual.Window\nThe PsychoPy window which provides screen dimensions for normalization.\nrequired\n\n\np\ntuple\nThe PsychoPy pixel coordinates to convert as (x, y). Origin is at screen center, x increases rightward, y increases upward.\nrequired",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "pix2tobii"
    ]
  },
  {
    "objectID": "api/pix2tobii.html#returns",
    "href": "api/pix2tobii.html#returns",
    "title": "",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nThe converted Tobii ADCS coordinates as (x, y) in range [0, 1]. Origin is top-left, x increases rightward, y increases downward.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "pix2tobii"
    ]
  },
  {
    "objectID": "api/pix2tobii.html#notes",
    "href": "api/pix2tobii.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\nThe conversion involves: 1. Translating the origin from center to top-left (+0.5 offset) 2. Normalizing by screen dimensions to get [0, 1] range 3. Inverting the Y-axis to match Tobii‚Äôs top-down convention\nThis function assumes PsychoPy‚Äôs pixel coordinate convention where (0, 0) is at screen center.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "pix2tobii"
    ]
  },
  {
    "objectID": "api/get_tobii_pos.html",
    "href": "api/get_tobii_pos.html",
    "title": "",
    "section": "",
    "text": "Coordinate Conversionget_tobii_pos",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_tobii_pos"
    ]
  },
  {
    "objectID": "api/get_tobii_pos.html#parameters",
    "href": "api/get_tobii_pos.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwin\npsychopy.visual.Window\nThe PsychoPy window which provides information about units and size. Window properties determine the source coordinate system.\nrequired\n\n\np\ntuple\nThe PsychoPy coordinates to convert as (x, y) in specified units.\nrequired\n\n\nunits\nstr\nThe units of the input PsychoPy coordinates. If None, uses the window‚Äôs default units. Supported: ‚Äònorm‚Äô, ‚Äòheight‚Äô, ‚Äòpix‚Äô, ‚Äòcm‚Äô, ‚Äòdeg‚Äô, ‚ÄòdegFlat‚Äô, ‚ÄòdegFlatPos‚Äô.\nNone",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_tobii_pos"
    ]
  },
  {
    "objectID": "api/get_tobii_pos.html#returns",
    "href": "api/get_tobii_pos.html#returns",
    "title": "",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nThe converted Tobii ADCS coordinates as (x, y) where both values are in range [0, 1]. (0, 0) is top-left, (1, 1) is bottom-right.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_tobii_pos"
    ]
  },
  {
    "objectID": "api/get_tobii_pos.html#raises",
    "href": "api/get_tobii_pos.html#raises",
    "title": "",
    "section": "Raises",
    "text": "Raises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the provided units are not supported.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_tobii_pos"
    ]
  },
  {
    "objectID": "api/get_tobii_pos.html#notes",
    "href": "api/get_tobii_pos.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\nThis function is the inverse of get_psychopy_pos() and is primarily used during calibration to inform the eye tracker where calibration targets are displayed on screen.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_tobii_pos"
    ]
  },
  {
    "objectID": "api/get_psychopy_pos.html",
    "href": "api/get_psychopy_pos.html",
    "title": "",
    "section": "",
    "text": "Coordinate Conversionget_psychopy_pos",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_psychopy_pos"
    ]
  },
  {
    "objectID": "api/get_psychopy_pos.html#parameters",
    "href": "api/get_psychopy_pos.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwin\npsychopy.visual.Window\nThe PsychoPy window which provides information about units and size. Window properties determine the target coordinate system.\nrequired\n\n\np\ntuple or array - like\nThe Tobii ADCS coordinates to convert. Can be: - Single coordinate: (x, y) tuple - Multiple coordinates: (N, 2) array where N is number of samples Values should be in range [0, 1] where (0, 0) is top-left and (1, 1) is bottom-right.\nrequired\n\n\nunits\nstr\nThe target units for the PsychoPy coordinates. If None, uses the window‚Äôs default units. Supported: ‚Äònorm‚Äô, ‚Äòheight‚Äô, ‚Äòpix‚Äô, ‚Äòcm‚Äô, ‚Äòdeg‚Äô, ‚ÄòdegFlat‚Äô, ‚ÄòdegFlatPos‚Äô.\nNone",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_psychopy_pos"
    ]
  },
  {
    "objectID": "api/get_psychopy_pos.html#returns",
    "href": "api/get_psychopy_pos.html#returns",
    "title": "",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple or ndarray\nThe converted PsychoPy coordinates in the specified unit system. - Single input: returns (x, y) tuple - Array input: returns (N, 2) array Origin is at screen center for most unit systems.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_psychopy_pos"
    ]
  },
  {
    "objectID": "api/get_psychopy_pos.html#raises",
    "href": "api/get_psychopy_pos.html#raises",
    "title": "",
    "section": "Raises",
    "text": "Raises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the provided units are not supported by PsychoPy.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_psychopy_pos"
    ]
  },
  {
    "objectID": "api/get_psychopy_pos.html#examples",
    "href": "api/get_psychopy_pos.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\n&gt;&gt;&gt; # Single coordinate\n&gt;&gt;&gt; pos = get_psychopy_pos(win, (0.5, 0.5))  # Returns (0, 0) in most units\n&gt;&gt;&gt; # Multiple coordinates (vectorized)\n&gt;&gt;&gt; coords = np.array([[0.5, 0.5], [0.0, 0.0], [1.0, 1.0]])\n&gt;&gt;&gt; positions = get_psychopy_pos(win, coords)  # Returns (N, 2) array",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_psychopy_pos"
    ]
  },
  {
    "objectID": "api/TobiiCalibrationSession.html",
    "href": "api/TobiiCalibrationSession.html",
    "title": "",
    "section": "",
    "text": "Internal CalibrationTobiiCalibrationSession",
    "crumbs": [
      "Reference",
      "Internal Calibration",
      "TobiiCalibrationSession"
    ]
  },
  {
    "objectID": "api/TobiiCalibrationSession.html#methods",
    "href": "api/TobiiCalibrationSession.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nrun\nMain routine to run the full Tobii calibration workflow.\n\n\n\n\nrun\nTobiiCalibrationSession.run(calibration_points)\nMain routine to run the full Tobii calibration workflow.\nThis function presents each calibration target, collects gaze data via the eye tracker, shows the results, and allows the user to retry any subset of points until satisfied.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncalibration_points\nlist of (float, float)\nList of calibration points in NORMALIZED coordinates [-1, 1]. Will be converted to window units and then to Tobii ADCS format.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nTrue if calibration was successful and accepted by user, False if aborted via escape key or if calibration computation failed.",
    "crumbs": [
      "Reference",
      "Internal Calibration",
      "TobiiCalibrationSession"
    ]
  },
  {
    "objectID": "api/MouseCalibrationSession.html",
    "href": "api/MouseCalibrationSession.html",
    "title": "",
    "section": "",
    "text": "Internal CalibrationMouseCalibrationSession",
    "crumbs": [
      "Reference",
      "Internal Calibration",
      "MouseCalibrationSession"
    ]
  },
  {
    "objectID": "api/MouseCalibrationSession.html#methods",
    "href": "api/MouseCalibrationSession.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\nrun\nMain function to run the mouse-based calibration routine.\n\n\n\n\nrun\nMouseCalibrationSession.run(calibration_points)\nMain function to run the mouse-based calibration routine.\nExecutes the complete calibration workflow using mouse position as a proxy for gaze data. Follows the same interaction pattern as Tobii calibration to ensure consistency across modes.\n\nParameters\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncalibration_points\nlist of (float, float)\nList of calibration points in NORMALIZED coordinates [-1, 1]. Will be converted to window units automatically.\nrequired\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nTrue if calibration finished successfully and was accepted by user, False if the user exits early via escape key.",
    "crumbs": [
      "Reference",
      "Internal Calibration",
      "MouseCalibrationSession"
    ]
  },
  {
    "objectID": "api/ETracker.start_recording.html",
    "href": "api/ETracker.start_recording.html",
    "title": "",
    "section": "",
    "text": "Main Eye-tracker ClassControl the RecordingETracker.start_recording",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.start_recording"
    ]
  },
  {
    "objectID": "api/ETracker.start_recording.html#parameters",
    "href": "api/ETracker.start_recording.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nOutput filename for gaze data. If None, generates timestamp-based name. File extension determines format (.h5/.hdf5 for HDF5, .csv for CSV, defaults to .h5).\nNone\n\n\nraw_format\nbool\nIf True, preserves all original Tobii SDK column names and data. If False (default), uses simplified column names and subset of columns. Raw format is useful for advanced analysis requiring full metadata.\nFalse",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.start_recording"
    ]
  },
  {
    "objectID": "api/ETracker.start_recording.html#examples",
    "href": "api/ETracker.start_recording.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\n### Standard format (simplified columns)\nET_controller.start_recording('data.h5')\n\n### Raw format (all Tobii SDK columns preserved)\nET_controller.start_recording('data_raw.h5', raw_format=True)",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.start_recording"
    ]
  },
  {
    "objectID": "api/ETracker.save_data.html",
    "href": "api/ETracker.save_data.html",
    "title": "",
    "section": "",
    "text": "Main Eye-tracker ClassControl the RecordingETracker.save_data",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.save_data"
    ]
  },
  {
    "objectID": "api/ETracker.save_data.html#notes",
    "href": "api/ETracker.save_data.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\n\nAutomatically called by stop_recording()\nSafe to call during active recording\nClears buffers after saving\nEvents are matched to nearest gaze sample by timestamp",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.save_data"
    ]
  },
  {
    "objectID": "api/ETracker.save_data.html#examples",
    "href": "api/ETracker.save_data.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\n# Automatic saving (most common)\nET_controller.start_recording('data.h5')\n# ... run experiment ...\nET_controller.stop_recording()  # Automatically calls save_data()\n\n# Manual periodic saves for long experiments\nET_controller.start_recording('long_experiment.h5')\n\nfor trial in range(100):\n    ET_controller.record_event(f'trial_{trial}_start')\n    # ... present stimuli ...\n    ET_controller.record_event(f'trial_{trial}_end')\n    \n    # Save data every 10 trials to prevent memory buildup\n    if (trial + 1) % 10 == 0:\n        ET_controller.save_data()  # Saves and clears buffers\n\nET_controller.stop_recording()\n\n# Save data at natural break points\nET_controller.start_recording('session.h5')\n\n# Block 1\nfor trial in range(20):\n    # ... run trial ...\n    pass\nET_controller.save_data()  # Save after block 1\n\n# Short break\ncore.wait(30)\n\n# Block 2\nfor trial in range(20):\n    # ... run trial ...\n    pass\nET_controller.save_data()  # Save after block 2\n\nET_controller.stop_recording()",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.save_data"
    ]
  },
  {
    "objectID": "api/ETracker.record_event.html",
    "href": "api/ETracker.record_event.html",
    "title": "",
    "section": "",
    "text": "Main Eye-tracker ClassControl the RecordingETracker.record_event",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.record_event"
    ]
  },
  {
    "objectID": "api/ETracker.record_event.html#parameters",
    "href": "api/ETracker.record_event.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlabel\nstr\nDescriptive label for the event (e.g., ‚Äòtrial_start‚Äô, ‚Äòstimulus_onset‚Äô).\nrequired",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.record_event"
    ]
  },
  {
    "objectID": "api/ETracker.record_event.html#raises",
    "href": "api/ETracker.record_event.html#raises",
    "title": "",
    "section": "Raises",
    "text": "Raises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nRuntimeWarning\nIf called when recording is not active.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.record_event"
    ]
  },
  {
    "objectID": "api/ETracker.record_event.html#examples",
    "href": "api/ETracker.record_event.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\nET_controller.record_event('trial_1_start')\n#  present stimulus \nET_controller.record_event('stimulus_offset')",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.record_event"
    ]
  },
  {
    "objectID": "api/ETracker.load_calibration.html",
    "href": "api/ETracker.load_calibration.html",
    "title": "",
    "section": "",
    "text": "Main Eye-tracker ClassCalibrationETracker.load_calibration",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.load_calibration"
    ]
  },
  {
    "objectID": "api/ETracker.load_calibration.html#parameters",
    "href": "api/ETracker.load_calibration.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nThe path to the calibration data file (e.g., ‚Äúsubject_01_calib.dat‚Äù). If use_gui is True, this path is used as the default suggestion in the file dialog. If use_gui is False, this parameter is required.\nNone\n\n\nuse_gui\nbool\nIf True, a graphical file-open dialog is displayed for the user to select the calibration file. Defaults to False.\nFalse",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.load_calibration"
    ]
  },
  {
    "objectID": "api/ETracker.load_calibration.html#returns",
    "href": "api/ETracker.load_calibration.html#returns",
    "title": "",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nReturns True if the calibration was successfully loaded and applied, and False otherwise (e.g., user cancelled the dialog, file not found, or data was invalid).",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.load_calibration"
    ]
  },
  {
    "objectID": "api/ETracker.load_calibration.html#raises",
    "href": "api/ETracker.load_calibration.html#raises",
    "title": "",
    "section": "Raises",
    "text": "Raises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nRuntimeError\nIf the method is called while the ETracker is in simulation mode.\n\n\n\nValueError\nIf use_gui is False and filename is not provided.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.load_calibration"
    ]
  },
  {
    "objectID": "api/ETracker.load_calibration.html#examples",
    "href": "api/ETracker.load_calibration.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\n# Load calibration from specific file\nsuccess = ET_controller.load_calibration('subject_01_calib.dat')\nif success:\n    ET_controller.start_recording('subject_01_data.h5')\n\n# Use GUI to select file\nsuccess = ET_controller.load_calibration(use_gui=True)\n\n# Multi-session workflow\n# Session 1: Calibrate and save\nET_controller.calibrate(5)\nET_controller.save_calibration('participant_123.dat')\nET_controller.start_recording('session_1.h5')\n# ... run experiment ...\nET_controller.stop_recording()\n\n# Session 2: Load previous calibration\nET_controller.load_calibration('participant_123.dat')\nET_controller.start_recording('session_2.h5')\n# ... run experiment ...\nET_controller.stop_recording()",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.load_calibration"
    ]
  },
  {
    "objectID": "api/ETracker.gaze_contingent.html",
    "href": "api/ETracker.gaze_contingent.html",
    "title": "",
    "section": "",
    "text": "Main Eye-tracker ClassGaze ContingentETracker.gaze_contingent",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Gaze Contingent",
      "ETracker.gaze_contingent"
    ]
  },
  {
    "objectID": "api/ETracker.gaze_contingent.html#parameters",
    "href": "api/ETracker.gaze_contingent.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nN\nint\nNumber of recent gaze samples to store in the rolling buffer. Larger values provide smoother estimates but increase latency. Typical values: 3-10 samples. Default 5.\n5",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Gaze Contingent",
      "ETracker.gaze_contingent"
    ]
  },
  {
    "objectID": "api/ETracker.gaze_contingent.html#raises",
    "href": "api/ETracker.gaze_contingent.html#raises",
    "title": "",
    "section": "Raises",
    "text": "Raises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf N is not an integer.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Gaze Contingent",
      "ETracker.gaze_contingent"
    ]
  },
  {
    "objectID": "api/ETracker.gaze_contingent.html#notes",
    "href": "api/ETracker.gaze_contingent.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\n\nCall this method ONCE before your experimental loop\nBuffer size trades off stability vs.¬†latency:\nSmaller N (3-5): Lower latency, more noise\nLarger N (8-10): Smoother tracking, higher latency\nFor 120 Hz eye tracker with N=5: ~42ms latency\nFor 60 Hz eye tracker with N=5: ~83ms latency",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Gaze Contingent",
      "ETracker.gaze_contingent"
    ]
  },
  {
    "objectID": "api/ETracker.gaze_contingent.html#examples",
    "href": "api/ETracker.gaze_contingent.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\n# Basic real-time gaze tracking\nET_controller.gaze_contingent(N=5)  # Initialize buffer\nET_controller.start_recording('data.h5')\n\n# Create gaze-contingent stimulus\ncircle = visual.Circle(win, radius=0.05, fillColor='red')\n\nfor frame in range(600):  # 10 seconds at 60 fps\n    gaze_pos = ET_controller.get_gaze_position()\n    circle.pos = gaze_pos\n    circle.draw()\n    win.flip()\n\nET_controller.stop_recording()\n\n# Adjust buffer size for your needs\nET_controller.gaze_contingent(N=3)   # Low latency, more jitter\nET_controller.gaze_contingent(N=10)  # Smooth, higher latency\n\n# Gaze-contingent window paradigm\nET_controller.gaze_contingent(N=5)\nET_controller.start_recording('gaze_window.h5')\n\nstimulus = visual.ImageStim(win, 'image.png')\nwindow = visual.Circle(win, radius=0.1, fillColor=None, lineColor='white')\n\nfor trial in range(20):\n    stimulus.draw()\n    \n    for frame in range(120):  # 2 seconds\n        gaze_pos = ET_controller.get_gaze_position()\n        window.pos = gaze_pos\n        window.draw()\n        win.flip()\n    \n    ET_controller.record_event(f'trial_{trial}_end')\n\nET_controller.stop_recording()",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Gaze Contingent",
      "ETracker.gaze_contingent"
    ]
  },
  {
    "objectID": "api/ETSettings.UIElementSizes.html",
    "href": "api/ETSettings.UIElementSizes.html",
    "title": "",
    "section": "",
    "text": "Configuration SettingsETSettings.UIElementSizes",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.UIElementSizes"
    ]
  },
  {
    "objectID": "api/ETSettings.UIElementSizes.html#attributes",
    "href": "api/ETSettings.UIElementSizes.html#attributes",
    "title": "",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nType\nDescription\n\n\n\n\nhighlight\nfloat\nRadius of circles highlighting selected calibration points for retry. Default is 0.02 (2% of screen height).\n\n\nline_width\nfloat\nThickness of lines drawn in calibration visualizations. Default is 0.003 (0.3% of screen height).\n\n\nmarker\nfloat\nSize of markers indicating data collection points. Default is 0.02 (2% of screen height).\n\n\nborder\nfloat\nThickness of the red calibration mode border around the screen. Default is 0.005 (0.5% of screen height).\n\n\nplot_line\nfloat\nWidth of lines in calibration result plots connecting targets to samples. Default is 0.002 (0.2% of screen height).\n\n\ntext\nfloat\nBase text height (deprecated - use specific text sizes below). Default is 0.025 (2.5% of screen height).\n\n\ntarget_circle\nfloat\nRadius of target circles drawn in calibration result visualizations. Default is 0.012 (1.2% of screen height).\n\n\ntarget_circle_width\nfloat\nLine width for target circle outlines in result visualizations. Default is 0.003 (0.3% of screen height).\n\n\nsample_marker\nfloat\nRadius of sample markers in circle visualization style. Default is 0.005 (0.5% of screen height).\n\n\ninstruction_text\nfloat\nText height for instruction displays during calibration. Default is 0.019 (1.9% of screen height).\n\n\nmessage_text\nfloat\nText height for general message displays. Default is 0.016 (1.6% of screen height).\n\n\ntitle_text\nfloat\nText height for title text in message boxes. Default is 0.018 (1.8% of screen height).\n\n\nlegend_text\nfloat\nText height for legend labels showing eye color coding. Default is 0.015 (1.5% of screen height).",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.UIElementSizes"
    ]
  },
  {
    "objectID": "api/ETSettings.UIElementSizes.html#notes",
    "href": "api/ETSettings.UIElementSizes.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\nHeight units provide consistent visual appearance across different screen sizes and aspect ratios. The conversion to pixels or other units is handled automatically by the coordinate conversion functions.",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.UIElementSizes"
    ]
  },
  {
    "objectID": "api/ETSettings.UIElementSizes.html#examples",
    "href": "api/ETSettings.UIElementSizes.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\n&gt;&gt;&gt; ui_sizes = UIElementSizes()\n&gt;&gt;&gt; ui_sizes.highlight = 0.06  # Larger highlight circles\n&gt;&gt;&gt; ui_sizes.instruction_text = 0.025  # Larger instructions",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.UIElementSizes"
    ]
  },
  {
    "objectID": "api/ETSettings.CalibrationColors.html",
    "href": "api/ETSettings.CalibrationColors.html",
    "title": "",
    "section": "",
    "text": "Configuration SettingsETSettings.CalibrationColors",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.CalibrationColors"
    ]
  },
  {
    "objectID": "api/ETSettings.CalibrationColors.html#attributes",
    "href": "api/ETSettings.CalibrationColors.html#attributes",
    "title": "",
    "section": "Attributes",
    "text": "Attributes\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\nleft_eye\ntuple of int\nRGBA color for Tobii left eye gaze samples (R, G, B, A). Default is (0, 255, 0, 255) - bright green.\n\n\nright_eye\ntuple of int\nRGBA color for Tobii right eye gaze samples (R, G, B, A). Default is (255, 0, 0, 255) - bright red.\n\n\nmouse\ntuple of int\nRGBA color for simulated mouse position samples (R, G, B, A). Default is (255, 128, 0, 255) - orange.\n\n\ntarget_outline\ntuple of int\nRGBA color for calibration target circle outlines (R, G, B, A). Default is (24, 24, 24, 255) - dark gray/black.\n\n\nhighlight\ntuple of int\nRGBA color for highlighting selected calibration points (R, G, B, A). Default is (255, 255, 0, 255) - bright yellow.",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.CalibrationColors"
    ]
  },
  {
    "objectID": "api/ETSettings.CalibrationColors.html#notes",
    "href": "api/ETSettings.CalibrationColors.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\nAll color values use 8-bit channels (0-255 range) in RGBA format. The alpha channel (A) controls opacity where 255 is fully opaque.",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.CalibrationColors"
    ]
  },
  {
    "objectID": "api/ETSettings.CalibrationColors.html#examples",
    "href": "api/ETSettings.CalibrationColors.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\n&gt;&gt;&gt; colors = CalibrationColors()\n&gt;&gt;&gt; colors.highlight = (0, 255, 255, 255)  # Change to cyan\n&gt;&gt;&gt; colors.left_eye = (0, 200, 0, 200)  # Semi-transparent green",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.CalibrationColors"
    ]
  },
  {
    "objectID": "api/BaseCalibrationSession.html",
    "href": "api/BaseCalibrationSession.html",
    "title": "",
    "section": "",
    "text": "Internal CalibrationBaseCalibrationSession",
    "crumbs": [
      "Reference",
      "Internal Calibration",
      "BaseCalibrationSession"
    ]
  },
  {
    "objectID": "api/BaseCalibrationSession.html#methods",
    "href": "api/BaseCalibrationSession.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\ncheck_points\nEnsure number of calibration points is within allowed range.\n\n\nshow_message_and_wait\nDisplay a message on screen and in console, then wait for keypress.\n\n\n\n\ncheck_points\nBaseCalibrationSession.check_points(calibration_points)\nEnsure number of calibration points is within allowed range.\nValidates that the provided calibration points fall within the supported range for infant calibration protocols. Both Tobii and simulation modes support 2-9 calibration points.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncalibration_points\nlist\nList of calibration point coordinates to validate.\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf number of points is less than 2 or greater than 9.\n\n\n\n\n\n\nshow_message_and_wait\nBaseCalibrationSession.show_message_and_wait(body, title='', pos=(0, -0.15))\nDisplay a message on screen and in console, then wait for keypress.\nShows formatted message both in the PsychoPy window and console output, enforces a minimum display time for readability and system stabilization, then pauses execution until any key is pressed.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbody\nstr\nThe main message text to display.\nrequired\n\n\ntitle\nstr\nTitle for the message box. Default empty string.\n''\n\n\npos\ntuple\nPosition of the message box center on screen. Default (0, -0.15).\n(0, -0.15)",
    "crumbs": [
      "Reference",
      "Internal Calibration",
      "BaseCalibrationSession"
    ]
  },
  {
    "objectID": "api/ETSettings.AnimationSettings.html",
    "href": "api/ETSettings.AnimationSettings.html",
    "title": "",
    "section": "",
    "text": "Configuration SettingsETSettings.AnimationSettings",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.AnimationSettings"
    ]
  },
  {
    "objectID": "api/ETSettings.AnimationSettings.html#attributes",
    "href": "api/ETSettings.AnimationSettings.html#attributes",
    "title": "",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nType\nDescription\n\n\n\n\nfocus_time\nfloat\nWait time in seconds before collecting calibration data at each point. Allows participant to fixate on the target. Default is 0.5 seconds.\n\n\nzoom_speed\nfloat\nSpeed multiplier for the zoom animation. Higher values make the size oscillation faster. Default is 6.0.\n\n\nmax_zoom_size_big\nfloat\nMaximum size for zoom animation (big preset) as percentage of screen height. Recommended for infants/children. Default is 0.16 (16% of screen height).\n\n\nmin_zoom_size_big\nfloat\nMinimum size for zoom animation (big preset) as percentage of screen height. Default is 0.05 (5% of screen height).\n\n\ntrill_size_big\nfloat\nFixed size for trill animation (big preset) as percentage of screen height. Default is 0.14 (14% of screen height).\n\n\nmax_zoom_size_small\nfloat\nMaximum size for zoom animation (small preset) as percentage of screen height. Recommended for adults. Default is 0.06 (6% of screen height).\n\n\nmin_zoom_size_small\nfloat\nMinimum size for zoom animation (small preset) as percentage of screen height. Default is 0.01 (1% of screen height).\n\n\ntrill_size_small\nfloat\nFixed size for trill animation (small preset) as percentage of screen height. Default is 0.04 (4% of screen height).\n\n\ntrill_rotation_range\nfloat\nMaximum rotation angle in degrees for trill animation. Default is 20 degrees.\n\n\ntrill_cycle_duration\nfloat\nTotal cycle time for trill animation in seconds (active + pause). Default is 1.5 seconds.\n\n\ntrill_active_duration\nfloat\nDuration of active trill rotation in seconds, within each cycle. Default is 1.1 seconds (leaves 0.4s pause).\n\n\ntrill_frequency\nfloat\nNumber of back-and-forth rotation oscillations per second during active trill phase. Default is 3.0 oscillations/second.",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.AnimationSettings"
    ]
  },
  {
    "objectID": "api/ETSettings.AnimationSettings.html#examples",
    "href": "api/ETSettings.AnimationSettings.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\n&gt;&gt;&gt; settings = AnimationSettings()\n&gt;&gt;&gt; settings.max_zoom_size_big = 0.18  # Increase big max size to 18%\n&gt;&gt;&gt; settings.trill_frequency = 5.0  # Faster trill",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.AnimationSettings"
    ]
  },
  {
    "objectID": "api/ETSettings.CalibrationPatterns.html",
    "href": "api/ETSettings.CalibrationPatterns.html",
    "title": "",
    "section": "",
    "text": "Configuration SettingsETSettings.CalibrationPatterns",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.CalibrationPatterns"
    ]
  },
  {
    "objectID": "api/ETSettings.CalibrationPatterns.html#attributes",
    "href": "api/ETSettings.CalibrationPatterns.html#attributes",
    "title": "",
    "section": "Attributes",
    "text": "Attributes\n\n\n\nName\nType\nDescription\n\n\n\n\npoints_5\nlist of tuple\n5-point calibration pattern (4 corners + center). Standard for quick calibrations with good coverage. Pattern: corners at ÔøΩ0.4 from edges to avoid screen boundaries.\n\n\npoints_9\nlist of tuple\n9-point calibration pattern (3ÔøΩ3 grid). Standard for comprehensive calibrations requiring high accuracy. Pattern: 3 rows ÔøΩ 3 columns with ÔøΩ0.4 positioning.\n\n\nnum_samples_mouse\nint\nNumber of mouse position samples to collect per calibration point in simulation mode. Default 5.",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.CalibrationPatterns"
    ]
  },
  {
    "objectID": "api/ETSettings.CalibrationPatterns.html#examples",
    "href": "api/ETSettings.CalibrationPatterns.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\n&gt;&gt;&gt; from DeToX import ETSettings as cfg\n&gt;&gt;&gt; from DeToX.Coords import norm_to_window_units\n&gt;&gt;&gt; \n&gt;&gt;&gt; # Change number of mouse samples collected per point\n&gt;&gt;&gt; cfg.calibration.num_samples_mouse = 10",
    "crumbs": [
      "Reference",
      "Configuration Settings",
      "ETSettings.CalibrationPatterns"
    ]
  },
  {
    "objectID": "api/ETracker.calibrate.html",
    "href": "api/ETracker.calibrate.html",
    "title": "",
    "section": "",
    "text": "Main Eye-tracker ClassCalibrationETracker.calibrate",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.calibrate"
    ]
  },
  {
    "objectID": "api/ETracker.calibrate.html#parameters",
    "href": "api/ETracker.calibrate.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncalibration_points\nint or list of tuple\nCalibration pattern specification. Use 5 for the standard 5-point pattern (4 corners + center; default). Use 9 for a comprehensive 9-point pattern (3x3 grid). Alternatively, provide a list of tuples with custom points in normalized coordinates [-1, 1]. Example: [(-0.4, 0.4), (0.4, 0.4), (0.0, 0.0)].\n5\n\n\ninfant_stims\nTrue or str or list or visual stimulus\nCalibration stimulus specification. Accepts multiple formats: True uses built-in stimuli from the package (default); False uses a default blue square; a str specifies a single image file path (e.g., 'stimulus.png'); a list of str provides multiple image paths; a PsychoPy visual stimulus (e.g., Circle, Rect, Polygon, ImageStim, ShapeStim) can be used as a single object; or a list of visual stimuli may be provided. If fewer stimuli than calibration points are given, they are automatically repeated and optionally shuffled to cover all points. Supported types: ImageStim, Circle, Rect, Polygon, ShapeStim. TextStim and MovieStim are not supported.\nTrue\n\n\nshuffle\nbool\nWhether to randomize stimulus presentation order after any necessary repetition. Helps prevent habituation to stimulus sequence. Default is True.\nTrue\n\n\naudio\nTrue or False or None or psychopy.sound.Sound\nControls attention-getting audio during calibration. True uses the built-in looping calibration sound (default). False or None disables audio. A psychopy.sound.Sound object may be provided for custom audio (ensure it is configured appropriately, e.g., loops=-1 for continuous looping). Audio plays when a calibration point is selected and fades out during data collection.\nTrue\n\n\nanim_type\n(zoom, trill)\nAnimation style for calibration stimuli. ‚Äòzoom‚Äô applies smooth size oscillation using a cosine function (default). ‚Äòtrill‚Äô uses rapid rotation with intermittent pauses.\n'zoom'\n\n\nstim_size\n(big, small)\nSize preset for calibration stimuli. ‚Äòbig‚Äô uses larger stimuli recommended for infants and children (default): zoom animation ranges from 5ÔøΩ16% screen height, trill uses 14%. ‚Äòsmall‚Äô uses smaller stimuli for adults: zoom ranges from 3ÔøΩ8%, trill uses 6%.\n'big'\n\n\nvisualization_style\n(circles, lines)\nHow to display calibration results. ‚Äòcircles‚Äô shows small filled circles at each gaze sample position. ‚Äòlines‚Äô draws lines from targets to gaze samples (default). In Tobii mode, green indicates the left eye and red the right eye; in simulation mode, orange represents mouse position.\n'circles'",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.calibrate"
    ]
  },
  {
    "objectID": "api/ETracker.calibrate.html#returns",
    "href": "api/ETracker.calibrate.html#returns",
    "title": "",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nTrue if calibration completed successfully and was accepted by the user. False if calibration was aborted (e.g., via ESC key) or failed.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.calibrate"
    ]
  },
  {
    "objectID": "api/ETracker.calibrate.html#raises",
    "href": "api/ETracker.calibrate.html#raises",
    "title": "",
    "section": "Raises",
    "text": "Raises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf calibration_points is not 5, 9, or a valid list of coordinate tuples; if visualization_style is not ‚Äòcircles‚Äô or ‚Äòlines‚Äô; or if infant_stims format is unrecognized.\n\n\n\nTypeError\nIf pre-loaded stimuli include unsupported types (e.g., TextStim, MovieStim).",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.calibrate"
    ]
  },
  {
    "objectID": "api/ETracker.calibrate.html#examples",
    "href": "api/ETracker.calibrate.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\nBasic usage with built-in stimuli:\ncontroller.calibrate(5)\n9-point calibration:\ncontroller.calibrate(9)\nCustom calibration points:\ncustom_points = [\n    (0.0, 0.0),      # Center\n    (-0.5, 0.5),     # Top-left\n    (0.5, 0.5),      # Top-right\n    (-0.5, -0.5),    # Bottom-left\n    (0.5, -0.5)      # Bottom-right\n]\ncontroller.calibrate(custom_points)\nSingle image file:\ncontroller.calibrate(5, infant_stims='my_stimulus.png')\nMultiple image files:\ncontroller.calibrate(5, infant_stims=['stim1.png', 'stim2.png', 'stim3.png'])\nSingle shape stimulus:\nred_square = visual.Rect(win, size=0.08, fillColor='red', units='height')\ncontroller.calibrate(5, infant_stims=red_square)\nMultiple shape stimuli:\nshapes = [\n    visual.Circle(win, radius=0.04, fillColor='red', units='height'),\n    visual.Rect(win, size=0.08, fillColor='blue', units='height'),\n    visual.Polygon(win, edges=6, radius=0.04, fillColor='green', units='height')\n]\ncontroller.calibrate(5, infant_stims=shapes, shuffle=True)\nCustom audio:\nfrom psychopy import sound\nmy_sound = sound.Sound('custom_beep.wav', loops=-1)\ncontroller.calibrate(5, audio=my_sound)\nNo audio with trill animation:\ncontroller.calibrate(5, audio=False, anim_type='trill')\nLines visualization style:\ncontroller.calibrate(5, visualization_style='lines')\nComplete custom workflow:\n# Position participant\ncontroller.show_status()\n\n# Custom calibration\nsuccess = controller.calibrate(\n    calibration_points=9,\n    infant_stims=['stim1.png', 'stim2.png'],\n    shuffle=True,\n    audio=True,\n    anim_type='zoom',\n    visualization_style='circles'\n)\n\nif success:\n    controller.start_recording('data.h5')\n    # ... run experiment ...\n    controller.stop_recording()",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.calibrate"
    ]
  },
  {
    "objectID": "api/ETracker.calibrate.html#notes",
    "href": "api/ETracker.calibrate.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\n\nCalibration uses normalized coordinates [-1, 1] where (0, 0) is screen center.\nStimuli are automatically repeated if fewer than calibration points.\nAnimation preserves stimulus aspect ratio automatically.\nIn simulation mode, use mouse to simulate gaze position.\nPress number keys (1ÔøΩ9) to select points, SPACE to collect data.\nPress ENTER to view results, ESC to abort.\nAfter viewing results: ENTER accepts, numbers + SPACE retries selected points.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.calibrate"
    ]
  },
  {
    "objectID": "api/ETracker.calibrate.html#see-also",
    "href": "api/ETracker.calibrate.html#see-also",
    "title": "",
    "section": "See Also",
    "text": "See Also\nshow_status : Position participant before calibration save_calibration : Save calibration for later sessions load_calibration : Load previously saved calibration",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.calibrate"
    ]
  },
  {
    "objectID": "api/ETracker.get_gaze_position.html",
    "href": "api/ETracker.get_gaze_position.html",
    "title": "",
    "section": "",
    "text": "Main Eye-tracker ClassGaze ContingentETracker.get_gaze_position",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Gaze Contingent",
      "ETracker.get_gaze_position"
    ]
  },
  {
    "objectID": "api/ETracker.get_gaze_position.html#parameters",
    "href": "api/ETracker.get_gaze_position.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfallback_offscreen\nbool\nIf True (default), returns an offscreen position (3x screen dimensions) when no valid gaze data is available. If False, returns None.\nTrue\n\n\nmethod\nstr\nAggregation method for combining samples and eyes. - ‚Äúmedian‚Äù (default): Robust to outliers, good for noisy data - ‚Äúmean‚Äù: Smoother but sensitive to outliers - ‚Äúlast‚Äù: Lowest latency, uses only most recent sample\n'median'",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Gaze Contingent",
      "ETracker.get_gaze_position"
    ]
  },
  {
    "objectID": "api/ETracker.get_gaze_position.html#returns",
    "href": "api/ETracker.get_gaze_position.html#returns",
    "title": "",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple or None\nGaze position (x, y) in PsychoPy coordinates (current window units), or None if no valid data and fallback_offscreen=False.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Gaze Contingent",
      "ETracker.get_gaze_position"
    ]
  },
  {
    "objectID": "api/ETracker.get_gaze_position.html#raises",
    "href": "api/ETracker.get_gaze_position.html#raises",
    "title": "",
    "section": "Raises",
    "text": "Raises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nRuntimeError\nIf gaze_contingent() was not called to initialize the buffer.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Gaze Contingent",
      "ETracker.get_gaze_position"
    ]
  },
  {
    "objectID": "api/ETracker.get_gaze_position.html#examples",
    "href": "api/ETracker.get_gaze_position.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\n# Basic usage (median aggregation)\npos = ET_controller.get_gaze_position()\nif pos is not None:\n     circle.pos = pos\n\n# Use mean for smoother tracking\npos = ET_controller.get_gaze_position(method=\"mean\")\n\n# Lowest latency (last sample only)\npos = ET_controller.get_gaze_position(method=\"last\")\n\n# Return None instead of offscreen position\npos = ET_controller.get_gaze_position(fallback_offscreen=False)\nif pos is None:\n     print(\"No valid gaze data\")",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Gaze Contingent",
      "ETracker.get_gaze_position"
    ]
  },
  {
    "objectID": "api/ETracker.html",
    "href": "api/ETracker.html",
    "title": "",
    "section": "",
    "text": "Main Eye-tracker ClassETracker",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "ETracker"
    ]
  },
  {
    "objectID": "api/ETracker.html#methods",
    "href": "api/ETracker.html#methods",
    "title": "",
    "section": "Methods",
    "text": "Methods\n\n\n\nName\nDescription\n\n\n\n\ncalibrate\nRun infant-friendly calibration procedure.\n\n\ngaze_contingent\nInitialize real-time gaze buffer for contingent applications.\n\n\nget_gaze_position\nGet current gaze position from rolling buffer.\n\n\nload_calibration\nLoads calibration data from a file and applies it to the eye tracker.\n\n\nrecord_event\nRecord timestamped experimental event during data collection.\n\n\nsave_calibration\nSave the current calibration data to a file.\n\n\nsave_data\nSave buffered gaze and event data to file with optimized processing.\n\n\nset_eyetracking_settings\nConfigure and apply Tobii eye tracker settings.\n\n\nshow_status\nReal-time visualization of participant‚Äôs eye position in track box.\n\n\nstart_recording\nBegin gaze data recording session.\n\n\nstop_recording\nStop gaze data recording and finalize session.\n\n\n\n\ncalibrate\nETracker.calibrate(\n    calibration_points=5,\n    infant_stims=True,\n    shuffle=True,\n    audio=True,\n    anim_type='zoom',\n    stim_size='big',\n    visualization_style='circles',\n)\nRun infant-friendly calibration procedure.\nPerforms eye tracker calibration using animated stimuli to engage infant participants. The calibration establishes the mapping between eye position and screen coordinates, which is essential for accurate gaze data collection. Automatically selects the appropriate calibration method based on operating mode (real eye tracker vs.¬†mouse simulation).\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ncalibration_points\nint or list of tuple\nCalibration pattern specification. Use 5 for the standard 5-point pattern (4 corners + center; default). Use 9 for a comprehensive 9-point pattern (3x3 grid). Alternatively, provide a list of tuples with custom points in normalized coordinates [-1, 1]. Example: [(-0.4, 0.4), (0.4, 0.4), (0.0, 0.0)].\n5\n\n\ninfant_stims\nTrue or str or list or visual stimulus\nCalibration stimulus specification. Accepts multiple formats: True uses built-in stimuli from the package (default); False uses a default blue square; a str specifies a single image file path (e.g., 'stimulus.png'); a list of str provides multiple image paths; a PsychoPy visual stimulus (e.g., Circle, Rect, Polygon, ImageStim, ShapeStim) can be used as a single object; or a list of visual stimuli may be provided. If fewer stimuli than calibration points are given, they are automatically repeated and optionally shuffled to cover all points. Supported types: ImageStim, Circle, Rect, Polygon, ShapeStim. TextStim and MovieStim are not supported.\nTrue\n\n\nshuffle\nbool\nWhether to randomize stimulus presentation order after any necessary repetition. Helps prevent habituation to stimulus sequence. Default is True.\nTrue\n\n\naudio\nTrue or False or None or psychopy.sound.Sound\nControls attention-getting audio during calibration. True uses the built-in looping calibration sound (default). False or None disables audio. A psychopy.sound.Sound object may be provided for custom audio (ensure it is configured appropriately, e.g., loops=-1 for continuous looping). Audio plays when a calibration point is selected and fades out during data collection.\nTrue\n\n\nanim_type\n(zoom, trill)\nAnimation style for calibration stimuli. ‚Äòzoom‚Äô applies smooth size oscillation using a cosine function (default). ‚Äòtrill‚Äô uses rapid rotation with intermittent pauses.\n'zoom'\n\n\nstim_size\n(big, small)\nSize preset for calibration stimuli. ‚Äòbig‚Äô uses larger stimuli recommended for infants and children (default): zoom animation ranges from 5ÔøΩ16% screen height, trill uses 14%. ‚Äòsmall‚Äô uses smaller stimuli for adults: zoom ranges from 3ÔøΩ8%, trill uses 6%.\n'big'\n\n\nvisualization_style\n(circles, lines)\nHow to display calibration results. ‚Äòcircles‚Äô shows small filled circles at each gaze sample position. ‚Äòlines‚Äô draws lines from targets to gaze samples (default). In Tobii mode, green indicates the left eye and red the right eye; in simulation mode, orange represents mouse position.\n'circles'\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nTrue if calibration completed successfully and was accepted by the user. False if calibration was aborted (e.g., via ESC key) or failed.\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf calibration_points is not 5, 9, or a valid list of coordinate tuples; if visualization_style is not ‚Äòcircles‚Äô or ‚Äòlines‚Äô; or if infant_stims format is unrecognized.\n\n\n\nTypeError\nIf pre-loaded stimuli include unsupported types (e.g., TextStim, MovieStim).\n\n\n\n\n\nExamples\nBasic usage with built-in stimuli:\ncontroller.calibrate(5)\n9-point calibration:\ncontroller.calibrate(9)\nCustom calibration points:\ncustom_points = [\n    (0.0, 0.0),      # Center\n    (-0.5, 0.5),     # Top-left\n    (0.5, 0.5),      # Top-right\n    (-0.5, -0.5),    # Bottom-left\n    (0.5, -0.5)      # Bottom-right\n]\ncontroller.calibrate(custom_points)\nSingle image file:\ncontroller.calibrate(5, infant_stims='my_stimulus.png')\nMultiple image files:\ncontroller.calibrate(5, infant_stims=['stim1.png', 'stim2.png', 'stim3.png'])\nSingle shape stimulus:\nred_square = visual.Rect(win, size=0.08, fillColor='red', units='height')\ncontroller.calibrate(5, infant_stims=red_square)\nMultiple shape stimuli:\nshapes = [\n    visual.Circle(win, radius=0.04, fillColor='red', units='height'),\n    visual.Rect(win, size=0.08, fillColor='blue', units='height'),\n    visual.Polygon(win, edges=6, radius=0.04, fillColor='green', units='height')\n]\ncontroller.calibrate(5, infant_stims=shapes, shuffle=True)\nCustom audio:\nfrom psychopy import sound\nmy_sound = sound.Sound('custom_beep.wav', loops=-1)\ncontroller.calibrate(5, audio=my_sound)\nNo audio with trill animation:\ncontroller.calibrate(5, audio=False, anim_type='trill')\nLines visualization style:\ncontroller.calibrate(5, visualization_style='lines')\nComplete custom workflow:\n# Position participant\ncontroller.show_status()\n\n# Custom calibration\nsuccess = controller.calibrate(\n    calibration_points=9,\n    infant_stims=['stim1.png', 'stim2.png'],\n    shuffle=True,\n    audio=True,\n    anim_type='zoom',\n    visualization_style='circles'\n)\n\nif success:\n    controller.start_recording('data.h5')\n    # ... run experiment ...\n    controller.stop_recording()\n\n\nNotes\n\nCalibration uses normalized coordinates [-1, 1] where (0, 0) is screen center.\nStimuli are automatically repeated if fewer than calibration points.\nAnimation preserves stimulus aspect ratio automatically.\nIn simulation mode, use mouse to simulate gaze position.\nPress number keys (1ÔøΩ9) to select points, SPACE to collect data.\nPress ENTER to view results, ESC to abort.\nAfter viewing results: ENTER accepts, numbers + SPACE retries selected points.\n\n\n\nSee Also\nshow_status : Position participant before calibration save_calibration : Save calibration for later sessions load_calibration : Load previously saved calibration\n\n\n\ngaze_contingent\nETracker.gaze_contingent(N=5)\nInitialize real-time gaze buffer for contingent applications.\nCreates a rolling buffer that stores the most recent N gaze samples, enabling real-time gaze-contingent paradigms. Must be called before using get_gaze_position() for real-time gaze tracking.\nThe buffer automatically maintains the N most recent samples, discarding older data. This provides a stable estimate of current gaze position by aggregating across multiple samples.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nN\nint\nNumber of recent gaze samples to store in the rolling buffer. Larger values provide smoother estimates but increase latency. Typical values: 3-10 samples. Default 5.\n5\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf N is not an integer.\n\n\n\n\n\nNotes\n\nCall this method ONCE before your experimental loop\nBuffer size trades off stability vs.¬†latency:\nSmaller N (3-5): Lower latency, more noise\nLarger N (8-10): Smoother tracking, higher latency\nFor 120 Hz eye tracker with N=5: ~42ms latency\nFor 60 Hz eye tracker with N=5: ~83ms latency\n\n\n\nExamples\n# Basic real-time gaze tracking\nET_controller.gaze_contingent(N=5)  # Initialize buffer\nET_controller.start_recording('data.h5')\n\n# Create gaze-contingent stimulus\ncircle = visual.Circle(win, radius=0.05, fillColor='red')\n\nfor frame in range(600):  # 10 seconds at 60 fps\n    gaze_pos = ET_controller.get_gaze_position()\n    circle.pos = gaze_pos\n    circle.draw()\n    win.flip()\n\nET_controller.stop_recording()\n\n# Adjust buffer size for your needs\nET_controller.gaze_contingent(N=3)   # Low latency, more jitter\nET_controller.gaze_contingent(N=10)  # Smooth, higher latency\n\n# Gaze-contingent window paradigm\nET_controller.gaze_contingent(N=5)\nET_controller.start_recording('gaze_window.h5')\n\nstimulus = visual.ImageStim(win, 'image.png')\nwindow = visual.Circle(win, radius=0.1, fillColor=None, lineColor='white')\n\nfor trial in range(20):\n    stimulus.draw()\n    \n    for frame in range(120):  # 2 seconds\n        gaze_pos = ET_controller.get_gaze_position()\n        window.pos = gaze_pos\n        window.draw()\n        win.flip()\n    \n    ET_controller.record_event(f'trial_{trial}_end')\n\nET_controller.stop_recording()\n\n\n\nget_gaze_position\nETracker.get_gaze_position(fallback_offscreen=True, method='median')\nGet current gaze position from rolling buffer.\nAggregates recent gaze samples from both eyes to provide a stable, real-time gaze estimate. Handles missing or invalid data gracefully.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfallback_offscreen\nbool\nIf True (default), returns an offscreen position (3x screen dimensions) when no valid gaze data is available. If False, returns None.\nTrue\n\n\nmethod\nstr\nAggregation method for combining samples and eyes. - ‚Äúmedian‚Äù (default): Robust to outliers, good for noisy data - ‚Äúmean‚Äù: Smoother but sensitive to outliers - ‚Äúlast‚Äù: Lowest latency, uses only most recent sample\n'median'\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple or None\nGaze position (x, y) in PsychoPy coordinates (current window units), or None if no valid data and fallback_offscreen=False.\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nRuntimeError\nIf gaze_contingent() was not called to initialize the buffer.\n\n\n\n\n\nExamples\n# Basic usage (median aggregation)\npos = ET_controller.get_gaze_position()\nif pos is not None:\n     circle.pos = pos\n\n# Use mean for smoother tracking\npos = ET_controller.get_gaze_position(method=\"mean\")\n\n# Lowest latency (last sample only)\npos = ET_controller.get_gaze_position(method=\"last\")\n\n# Return None instead of offscreen position\npos = ET_controller.get_gaze_position(fallback_offscreen=False)\nif pos is None:\n     print(\"No valid gaze data\")\n\n\n\nload_calibration\nETracker.load_calibration(filename=None, use_gui=False)\nLoads calibration data from a file and applies it to the eye tracker.\nThis method allows reusing a previously saved calibration, which can save significant time for participants, especially in multi-session studies. The calibration data must be a binary file generated by a Tobii eye tracker, typically via the save_calibration() method. This operation is only available when connected to a physical eye tracker.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nThe path to the calibration data file (e.g., ‚Äúsubject_01_calib.dat‚Äù). If use_gui is True, this path is used as the default suggestion in the file dialog. If use_gui is False, this parameter is required.\nNone\n\n\nuse_gui\nbool\nIf True, a graphical file-open dialog is displayed for the user to select the calibration file. Defaults to False.\nFalse\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nReturns True if the calibration was successfully loaded and applied, and False otherwise (e.g., user cancelled the dialog, file not found, or data was invalid).\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nRuntimeError\nIf the method is called while the ETracker is in simulation mode.\n\n\n\nValueError\nIf use_gui is False and filename is not provided.\n\n\n\n\n\nExamples\n# Load calibration from specific file\nsuccess = ET_controller.load_calibration('subject_01_calib.dat')\nif success:\n    ET_controller.start_recording('subject_01_data.h5')\n\n# Use GUI to select file\nsuccess = ET_controller.load_calibration(use_gui=True)\n\n# Multi-session workflow\n# Session 1: Calibrate and save\nET_controller.calibrate(5)\nET_controller.save_calibration('participant_123.dat')\nET_controller.start_recording('session_1.h5')\n# ... run experiment ...\nET_controller.stop_recording()\n\n# Session 2: Load previous calibration\nET_controller.load_calibration('participant_123.dat')\nET_controller.start_recording('session_2.h5')\n# ... run experiment ...\nET_controller.stop_recording()\n\n\n\nrecord_event\nETracker.record_event(label)\nRecord timestamped experimental event during data collection.\nEvents are merged with gaze data based on timestamp proximity during save operations. Uses appropriate timing source for simulation vs.¬†real eye tracker modes.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nlabel\nstr\nDescriptive label for the event (e.g., ‚Äòtrial_start‚Äô, ‚Äòstimulus_onset‚Äô).\nrequired\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nRuntimeWarning\nIf called when recording is not active.\n\n\n\n\n\nExamples\nET_controller.record_event('trial_1_start')\n#  present stimulus \nET_controller.record_event('stimulus_offset')\n\n\n\nsave_calibration\nETracker.save_calibration(filename=None, use_gui=False)\nSave the current calibration data to a file.\nRetrieves the active calibration data from the connected Tobii eye tracker and saves it as a binary file. This can be reloaded later with load_calibration() to avoid re-calibrating the same participant.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | None\nDesired output path. If None and use_gui is False, a timestamped default name is used (e.g., ‚ÄòYYYY-mm-dd_HH-MM-SS_calibration.dat‚Äô). If provided without an extension, ‚Äò.dat‚Äô is appended. If an extension is already present, it is left unchanged.\nNone\n\n\nuse_gui\nbool\nIf True, opens a file-save dialog (Psychopy) where the user chooses the path. The suggested name respects the logic above. Default False.\nFalse\n\n\n\n\n\nReturns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nTrue if saved successfully; False if cancelled, no data available, in simulation mode, or on error.\n\n\n\n\n\nNotes\n\nIn simulation mode, saving is skipped and a warning is issued.\nIf use_gui is True and the dialog is cancelled, returns False.\n\n\n\nExamples\n# Save with default timestamped name\nET_controller.save_calibration()\n\n### Save with specified filename\nET_controller.save_calibration('subject_01_calib.dat')\n\n\n\nsave_data\nETracker.save_data()\nSave buffered gaze and event data to file with optimized processing.\nUses thread-safe buffer swapping to minimize lock time, then processes and saves data in CSV or HDF5 format. Events are merged with gaze data based on timestamp proximity.\nThis method is typically called automatically by stop_recording(), but can be called manually during recording to periodically save data and clear buffers. This is useful for long experiments to avoid memory buildup and ensure data is saved even if the program crashes.\n\nNotes\n\nAutomatically called by stop_recording()\nSafe to call during active recording\nClears buffers after saving\nEvents are matched to nearest gaze sample by timestamp\n\n\n\nExamples\n# Automatic saving (most common)\nET_controller.start_recording('data.h5')\n# ... run experiment ...\nET_controller.stop_recording()  # Automatically calls save_data()\n\n# Manual periodic saves for long experiments\nET_controller.start_recording('long_experiment.h5')\n\nfor trial in range(100):\n    ET_controller.record_event(f'trial_{trial}_start')\n    # ... present stimuli ...\n    ET_controller.record_event(f'trial_{trial}_end')\n    \n    # Save data every 10 trials to prevent memory buildup\n    if (trial + 1) % 10 == 0:\n        ET_controller.save_data()  # Saves and clears buffers\n\nET_controller.stop_recording()\n\n# Save data at natural break points\nET_controller.start_recording('session.h5')\n\n# Block 1\nfor trial in range(20):\n    # ... run trial ...\n    pass\nET_controller.save_data()  # Save after block 1\n\n# Short break\ncore.wait(30)\n\n# Block 2\nfor trial in range(20):\n    # ... run trial ...\n    pass\nET_controller.save_data()  # Save after block 2\n\nET_controller.stop_recording()\n\n\n\nset_eyetracking_settings\nETracker.set_eyetracking_settings(\n    desired_fps=None,\n    desired_illumination_mode=None,\n    use_gui=False,\n)\nConfigure and apply Tobii eye tracker settings.\nThis method updates the eye tracker‚Äôs sampling frequency (FPS) and illumination mode, either programmatically or via a graphical interface. It ensures that configuration changes are only made when the device is idle and connected.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndesired_fps\nint\nDesired sampling frequency in Hz (e.g., 60, 120, 300). If None, the current frequency is retained.\nNone\n\n\ndesired_illumination_mode\nstr\nDesired illumination mode (e.g., ‚ÄòAuto‚Äô, ‚ÄòBright‚Äô, ‚ÄòDark‚Äô). If None, the current illumination mode is retained.\nNone\n\n\nuse_gui\nbool\nIf True, opens a PsychoPy GUI dialog that allows users to select settings interactively. Defaults to False.\nFalse\n\n\n\n\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nRuntimeError\nIf no physical eye tracker is connected or if the function is called in simulation mode.\n\n\n\nValueError\nIf the specified FPS or illumination mode is not supported by the connected device.\n\n\n\n\n\nNotes\n\nSettings cannot be changed during active recording. If an ongoing recording is detected, a non-blocking warning is issued and the function exits safely.\nWhen use_gui=True, a PsychoPy dialog window appears. It must be closed manually before the program continues.\nAfter successfully applying new settings, the internal attributes self.fps and self.illum_mode are updated to reflect the current device configuration.\n\n\n\nExamples\n# Set frequency to 120 Hz programmatically\nET_controller.set_eyetracking_settings(desired_fps=120)\n\n# Set illumination mode to 'Bright'\nET_controller.set_eyetracking_settings(desired_illumination_mode='Bright')\n\n# Set both frequency and illumination mode\nET_controller.set_eyetracking_settings(desired_fps=120, desired_illumination_mode='Bright')\n\n# Use GUI to select settings interactively\nET_controller.set_eyetracking_settings(use_gui=True)\n\n\n\nshow_status\nETracker.show_status(decision_key='space', video_help=True)\nReal-time visualization of participant‚Äôs eye position in track box.\nCreates interactive display showing left/right eye positions and distance from screen. Useful for positioning participants before data collection. Updates continuously until exit key is pressed.\nOptionally displays an instructional video in the background to help guide participant positioning. You can use the built-in video, disable the video, or provide your own custom MovieStim object.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndecision_key\nstr\nKey to press to exit visualization. Default ‚Äòspace‚Äô.\n'space'\n\n\nvideo_help\nbool or visual.MovieStim\nControls background video display: - True: Uses built-in instructional video (default) - False: No video displayed - visual.MovieStim: Uses your pre-loaded custom video. You are responsible for scaling (size) and positioning (pos) the MovieStim to fit your desired layout. Default True.\nTrue\n\n\n\n\n\nNotes\nIn simulation mode, use scroll wheel to adjust simulated distance. Eye positions shown as green (left) and red (right) circles.\nThe built-in video (when video_help=True) is sized at (1.06, 0.6) in height units and positioned at (0, -0.08) to avoid covering the track box.\n\n\nExamples\n# Basic usage with built-in video\nET_controller.show_status()\n\n# No background video\nET_controller.show_status(video_help=False)\n\n# Custom exit key\nET_controller.show_status(decision_key='return')\n\n# Use custom video\nfrom psychopy import visual\nmy_video = visual.MovieStim(\n    win, \n    'instructions.mp4', \n    size=(0.8, 0.6), \n    pos=(0, -0.1)\n)\nET_controller.show_status(video_help=my_video)\n\n# Complete workflow: position participant before calibration\nET_controller.show_status()  # Position participant\nsuccess = ET_controller.calibrate(5)  # Run calibration\nif success:\n    ET_controller.start_recording('data.h5')  # Start recording\n\n\n\nstart_recording\nETracker.start_recording(filename=None, raw_format=False)\nBegin gaze data recording session.\nInitializes file structure, clears any existing buffers, and starts data collection from either the eye tracker or simulation mode. Creates HDF5 or CSV files based on filename extension.\n\nParameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr\nOutput filename for gaze data. If None, generates timestamp-based name. File extension determines format (.h5/.hdf5 for HDF5, .csv for CSV, defaults to .h5).\nNone\n\n\nraw_format\nbool\nIf True, preserves all original Tobii SDK column names and data. If False (default), uses simplified column names and subset of columns. Raw format is useful for advanced analysis requiring full metadata.\nFalse\n\n\n\n\n\nExamples\n### Standard format (simplified columns)\nET_controller.start_recording('data.h5')\n\n### Raw format (all Tobii SDK columns preserved)\nET_controller.start_recording('data_raw.h5', raw_format=True)\n\n\n\nstop_recording\nETracker.stop_recording()\nStop gaze data recording and finalize session.\nPerforms complete shutdown: stops data collection, cleans up resources, saves all buffered data, and reports session summary. Handles both simulation and real eye tracker modes appropriately.\n\nRaises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUserWarning\nIf recording is not currently active.\n\n\n\n\n\nNotes\nAll pending data in buffers is automatically saved before completion. Recording duration is measured from start_recording() call.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "ETracker"
    ]
  },
  {
    "objectID": "api/ETracker.save_calibration.html",
    "href": "api/ETracker.save_calibration.html",
    "title": "",
    "section": "",
    "text": "Main Eye-tracker ClassCalibrationETracker.save_calibration",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.save_calibration"
    ]
  },
  {
    "objectID": "api/ETracker.save_calibration.html#parameters",
    "href": "api/ETracker.save_calibration.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nfilename\nstr | None\nDesired output path. If None and use_gui is False, a timestamped default name is used (e.g., ‚ÄòYYYY-mm-dd_HH-MM-SS_calibration.dat‚Äô). If provided without an extension, ‚Äò.dat‚Äô is appended. If an extension is already present, it is left unchanged.\nNone\n\n\nuse_gui\nbool\nIf True, opens a file-save dialog (Psychopy) where the user chooses the path. The suggested name respects the logic above. Default False.\nFalse",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.save_calibration"
    ]
  },
  {
    "objectID": "api/ETracker.save_calibration.html#returns",
    "href": "api/ETracker.save_calibration.html#returns",
    "title": "",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nbool\nTrue if saved successfully; False if cancelled, no data available, in simulation mode, or on error.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.save_calibration"
    ]
  },
  {
    "objectID": "api/ETracker.save_calibration.html#notes",
    "href": "api/ETracker.save_calibration.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\n\nIn simulation mode, saving is skipped and a warning is issued.\nIf use_gui is True and the dialog is cancelled, returns False.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.save_calibration"
    ]
  },
  {
    "objectID": "api/ETracker.save_calibration.html#examples",
    "href": "api/ETracker.save_calibration.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\n# Save with default timestamped name\nET_controller.save_calibration()\n\n### Save with specified filename\nET_controller.save_calibration('subject_01_calib.dat')",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.save_calibration"
    ]
  },
  {
    "objectID": "api/ETracker.show_status.html",
    "href": "api/ETracker.show_status.html",
    "title": "",
    "section": "",
    "text": "Main Eye-tracker ClassCalibrationETracker.show_status",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.show_status"
    ]
  },
  {
    "objectID": "api/ETracker.show_status.html#parameters",
    "href": "api/ETracker.show_status.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndecision_key\nstr\nKey to press to exit visualization. Default ‚Äòspace‚Äô.\n'space'\n\n\nvideo_help\nbool or visual.MovieStim\nControls background video display: - True: Uses built-in instructional video (default) - False: No video displayed - visual.MovieStim: Uses your pre-loaded custom video. You are responsible for scaling (size) and positioning (pos) the MovieStim to fit your desired layout. Default True.\nTrue",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.show_status"
    ]
  },
  {
    "objectID": "api/ETracker.show_status.html#notes",
    "href": "api/ETracker.show_status.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\nIn simulation mode, use scroll wheel to adjust simulated distance. Eye positions shown as green (left) and red (right) circles.\nThe built-in video (when video_help=True) is sized at (1.06, 0.6) in height units and positioned at (0, -0.08) to avoid covering the track box.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.show_status"
    ]
  },
  {
    "objectID": "api/ETracker.show_status.html#examples",
    "href": "api/ETracker.show_status.html#examples",
    "title": "",
    "section": "Examples",
    "text": "Examples\n# Basic usage with built-in video\nET_controller.show_status()\n\n# No background video\nET_controller.show_status(video_help=False)\n\n# Custom exit key\nET_controller.show_status(decision_key='return')\n\n# Use custom video\nfrom psychopy import visual\nmy_video = visual.MovieStim(\n    win, \n    'instructions.mp4', \n    size=(0.8, 0.6), \n    pos=(0, -0.1)\n)\nET_controller.show_status(video_help=my_video)\n\n# Complete workflow: position participant before calibration\nET_controller.show_status()  # Position participant\nsuccess = ET_controller.calibrate(5)  # Run calibration\nif success:\n    ET_controller.start_recording('data.h5')  # Start recording",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Calibration",
      "ETracker.show_status"
    ]
  },
  {
    "objectID": "api/ETracker.stop_recording.html",
    "href": "api/ETracker.stop_recording.html",
    "title": "",
    "section": "",
    "text": "Main Eye-tracker ClassControl the RecordingETracker.stop_recording",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.stop_recording"
    ]
  },
  {
    "objectID": "api/ETracker.stop_recording.html#raises",
    "href": "api/ETracker.stop_recording.html#raises",
    "title": "",
    "section": "Raises",
    "text": "Raises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nUserWarning\nIf recording is not currently active.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.stop_recording"
    ]
  },
  {
    "objectID": "api/ETracker.stop_recording.html#notes",
    "href": "api/ETracker.stop_recording.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\nAll pending data in buffers is automatically saved before completion. Recording duration is measured from start_recording() call.",
    "crumbs": [
      "Reference",
      "Main Eye-tracker Class",
      "Control the Recording",
      "ETracker.stop_recording"
    ]
  },
  {
    "objectID": "api/NicePrint.html",
    "href": "api/NicePrint.html",
    "title": "",
    "section": "",
    "text": "UtilitiesNicePrint",
    "crumbs": [
      "Reference",
      "Utilities",
      "NicePrint"
    ]
  },
  {
    "objectID": "api/NicePrint.html#parameters",
    "href": "api/NicePrint.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nbody\nstr\nThe string to print inside the box. Can contain multiple lines separated by newline characters. Each line will be padded to align within the box.\nrequired\n\n\ntitle\nstr\nA title to print on the top border of the box. The title will be centered within the top border. If empty string or not provided, the top border will be solid. Default empty string.\n''\n\n\nverbose\nbool\nIf True, the formatted box will be printed to the console. Default is True.\nTrue",
    "crumbs": [
      "Reference",
      "Utilities",
      "NicePrint"
    ]
  },
  {
    "objectID": "api/NicePrint.html#returns",
    "href": "api/NicePrint.html#returns",
    "title": "",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nstr\nThe formatted text with box characters, ready for display in console or use with PsychoPy TextStim objects. Includes all box-drawing characters and proper spacing.",
    "crumbs": [
      "Reference",
      "Utilities",
      "NicePrint"
    ]
  },
  {
    "objectID": "api/convert_height_to_units.html",
    "href": "api/convert_height_to_units.html",
    "title": "",
    "section": "",
    "text": "Coordinate Conversionconvert_height_to_units",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "convert_height_to_units"
    ]
  },
  {
    "objectID": "api/convert_height_to_units.html#parameters",
    "href": "api/convert_height_to_units.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwin\npsychopy.visual.Window\nThe PsychoPy window which provides information about units and size. The window‚Äôs current unit system determines the conversion method.\nrequired\n\n\nheight_value\nfloat\nSize in height units (fraction of screen height). For example, 0.1 represents 10% of the screen height.\nrequired",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "convert_height_to_units"
    ]
  },
  {
    "objectID": "api/convert_height_to_units.html#returns",
    "href": "api/convert_height_to_units.html#returns",
    "title": "",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\nfloat\nSize converted to current window units. The returned value maintains the same visual size on screen as the original height specification.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "convert_height_to_units"
    ]
  },
  {
    "objectID": "api/convert_height_to_units.html#notes",
    "href": "api/convert_height_to_units.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\nHeight units are PsychoPy‚Äôs recommended unit system for maintaining consistent appearance across different screen sizes and aspect ratios. This function enables that consistency when working with other unit systems.\nSupported unit conversions: - height: No conversion needed (identity transform) - norm: Scales by 2.0 to match normalized coordinate range - pix: Multiplies by screen height in pixels - cm/deg: Converts through pixels using monitor calibration",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "convert_height_to_units"
    ]
  },
  {
    "objectID": "api/get_psychopy_pos_from_trackbox.html",
    "href": "api/get_psychopy_pos_from_trackbox.html",
    "title": "",
    "section": "",
    "text": "Coordinate Conversionget_psychopy_pos_from_trackbox",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_psychopy_pos_from_trackbox"
    ]
  },
  {
    "objectID": "api/get_psychopy_pos_from_trackbox.html#parameters",
    "href": "api/get_psychopy_pos_from_trackbox.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwin\npsychopy.visual.Window\nThe PsychoPy window which provides information about units and size. Window properties determine the target coordinate system.\nrequired\n\n\np\ntuple\nThe Tobii TBCS coordinates to convert as (x, y). Values are in range [0, 1] representing position within the track box from the tracker‚Äôs perspective.\nrequired\n\n\nunits\nstr\nThe target units for the PsychoPy coordinates. If None, uses the window‚Äôs default units. Supported: ‚Äònorm‚Äô, ‚Äòheight‚Äô, ‚Äòpix‚Äô, ‚Äòcm‚Äô, ‚Äòdeg‚Äô, ‚ÄòdegFlat‚Äô, ‚ÄòdegFlatPos‚Äô.\nNone",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_psychopy_pos_from_trackbox"
    ]
  },
  {
    "objectID": "api/get_psychopy_pos_from_trackbox.html#returns",
    "href": "api/get_psychopy_pos_from_trackbox.html#returns",
    "title": "",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nThe converted PsychoPy coordinates in the specified unit system. Suitable for positioning visual feedback about user position.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_psychopy_pos_from_trackbox"
    ]
  },
  {
    "objectID": "api/get_psychopy_pos_from_trackbox.html#raises",
    "href": "api/get_psychopy_pos_from_trackbox.html#raises",
    "title": "",
    "section": "Raises",
    "text": "Raises\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf the provided units are not supported.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_psychopy_pos_from_trackbox"
    ]
  },
  {
    "objectID": "api/get_psychopy_pos_from_trackbox.html#notes",
    "href": "api/get_psychopy_pos_from_trackbox.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\nTBCS coordinates are primarily used in the show_status() method to provide visual feedback about participant positioning. The X-axis is reversed compared to ADCS because TBCS uses the tracker‚Äôs perspective.\nThis function handles the perspective reversal and transforms to PsychoPy‚Äôs coordinate conventions for proper visualization.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "get_psychopy_pos_from_trackbox"
    ]
  },
  {
    "objectID": "api/index.html#main-eye-tracker-class",
    "href": "api/index.html#main-eye-tracker-class",
    "title": "",
    "section": "Main Eye-tracker Class",
    "text": "Main Eye-tracker Class\nThe central class of the DeToX package, responsible for connecting to and managing the Tobii eye tracker. This class must be instantiated before any other functionality can be used. It provides access to calibration, recording control, and gaze-contingent presentation methods. Below are the main methods and properties of this class:\n\n\n\nETracker\nA high-level controller for running eye-tracking experiments with Tobii Pro and PsychoPy.\n\n\n\n\nControl the Recording\nFunctions for starting, stopping, and managing the capture of eye-tracking data. These methods allow you to initiate and terminate recordings, mark events of interest, and save collected data to disk.\n\n\n\nETracker.start_recording\nBegin gaze data recording session.\n\n\nETracker.stop_recording\nStop gaze data recording and finalize session.\n\n\nETracker.record_event\nRecord timestamped experimental event during data collection.\n\n\nETracker.save_data\nSave buffered gaze and event data to file with optimized processing.\n\n\n\n\n\nCalibration\nMethods for running and managing eye tracker calibration. These include displaying calibration status, initiating calibration routines, and saving or loading calibration settings to ensure accurate gaze data.\n\n\n\nETracker.show_status\nReal-time visualization of participant‚Äôs eye position in track box.\n\n\nETracker.calibrate\nRun infant-friendly calibration procedure.\n\n\nETracker.save_calibration\nSave the current calibration data to a file.\n\n\nETracker.load_calibration\nLoads calibration data from a file and applies it to the eye tracker.\n\n\n\n\n\nGaze Contingent\nTools for running gaze-contingent experiments, where visual presentation adapts dynamically to a participants gaze position. Includes functionality for live gaze-contingent control and methods to compute average gaze positions for experimental logic.\n\n\n\nETracker.gaze_contingent\nInitialize real-time gaze buffer for contingent applications.\n\n\nETracker.get_gaze_position\nGet current gaze position from rolling buffer.",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "api/index.html#internal-calibration",
    "href": "api/index.html#internal-calibration",
    "title": "",
    "section": "Internal Calibration",
    "text": "Internal Calibration\nClasses for running eye-tracker calibration. These get called internally by the ETracker class, and are reported here only for completeness. They are comprise of a base calibration class that set the common interface, and the specific implementations for Tobii and Mouse calibration.\n\n\n\nBaseCalibrationSession\nBase class with common functionality for both calibration types.\n\n\nTobiiCalibrationSession\nTobii-based calibration session for real eye tracking.\n\n\nMouseCalibrationSession\nMouse-based calibration session for simulation mode.",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "api/index.html#coordinate-conversion",
    "href": "api/index.html#coordinate-conversion",
    "title": "",
    "section": "Coordinate Conversion",
    "text": "Coordinate Conversion\nA collection of utility functions designed to help the conversion of the data from different formats. These are called internally by the ETracker class, but can be used independently if needed.\n\n\n\nconvert_height_to_units\nConvert a size from height units to the current window units.\n\n\nget_psychopy_pos\nConvert Tobii ADCS coordinates to PsychoPy coordinates.\n\n\npsychopy_to_pixels\nConvert PsychoPy coordinates to pixel coordinates.\n\n\nget_tobii_pos\nConvert PsychoPy coordinates to Tobii ADCS coordinates.\n\n\npix2tobii\nConvert PsychoPy pixel coordinates to Tobii ADCS coordinates.\n\n\ntobii2pix\nConvert Tobii ADCS coordinates to PsychoPy pixel coordinates.\n\n\nget_psychopy_pos_from_trackbox\nConvert Tobii TBCS coordinates to PsychoPy coordinates.",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "api/index.html#utilities",
    "href": "api/index.html#utilities",
    "title": "",
    "section": "Utilities",
    "text": "Utilities\nA functions for presenting stimuli and formatted text. These are called internally by the ETracker class.\n\n\n\nNicePrint\nPrint a message in a box with an optional title AND return the formatted text.",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "api/index.html#configuration-settings",
    "href": "api/index.html#configuration-settings",
    "title": "",
    "section": "Configuration Settings",
    "text": "Configuration Settings\nConfiguration classes and module-level instances for customizing the behavior and appearance of the DeToX eye-tracking system. These settings control animation parameters, colors, UI element sizes, and data formats. Note: These settings should generally not be changed unless you have specific requirements. However, they can be modified through the module-level instances (e.g., ETSettings.animation) for easy configuration of your experiments.\n\n\n\nETSettings.AnimationSettings\nAnimation parameters for calibration stimuli.\n\n\nETSettings.CalibrationPatterns\nStandard calibration point patterns in normalized coordinates.\n\n\nETSettings.CalibrationColors\nColor settings for calibration visual elements.\n\n\nETSettings.UIElementSizes\nSize settings for user interface elements.",
    "crumbs": [
      "Reference"
    ]
  },
  {
    "objectID": "api/psychopy_to_pixels.html",
    "href": "api/psychopy_to_pixels.html",
    "title": "",
    "section": "",
    "text": "Coordinate Conversionpsychopy_to_pixels",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "psychopy_to_pixels"
    ]
  },
  {
    "objectID": "api/psychopy_to_pixels.html#parameters",
    "href": "api/psychopy_to_pixels.html#parameters",
    "title": "",
    "section": "Parameters",
    "text": "Parameters\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\nwin\npsychopy.visual.Window\nThe PsychoPy window which provides information about units and size. Window units and dimensions determine the conversion method.\nrequired\n\n\npos\ntuple\nThe PsychoPy coordinates to convert as (x, y) in current window units.\nrequired",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "psychopy_to_pixels"
    ]
  },
  {
    "objectID": "api/psychopy_to_pixels.html#returns",
    "href": "api/psychopy_to_pixels.html#returns",
    "title": "",
    "section": "Returns",
    "text": "Returns\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple\nThe converted pixel coordinates as (int, int) with origin at top-left. Values are rounded to nearest integer for pixel alignment.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "psychopy_to_pixels"
    ]
  },
  {
    "objectID": "api/psychopy_to_pixels.html#notes",
    "href": "api/psychopy_to_pixels.html#notes",
    "title": "",
    "section": "Notes",
    "text": "Notes\nThis function handles the main PsychoPy coordinate systems: - ‚Äòheight‚Äô: Screen height = 1, width adjusted by aspect ratio, centered origin - ‚Äònorm‚Äô: Screen ranges from -1 to 1 in both dimensions, centered origin - Other units: Assumes coordinates are already close to pixel values\nThe output uses standard image coordinates where (0,0) is top-left and y increases downward, suitable for PIL and similar libraries.",
    "crumbs": [
      "Reference",
      "Coordinate Conversion",
      "psychopy_to_pixels"
    ]
  },
  {
    "objectID": "Vignettes/Installation.html",
    "href": "Vignettes/Installation.html",
    "title": "Installation",
    "section": "",
    "text": "So you‚Äôre interested in using DeToX? Awesome! Let‚Äôs get you set up quickly.\nDeToX is designed as a lightweight wrapper around PsychoPy and tobii_research. Here‚Äôs the good news: tobii_research usually comes bundled with PsychoPy, which means the only real hurdle is installing PsychoPy itself. And yes, PsychoPy can be a bit tricky to install due to its many dependencies‚Äîbut don‚Äôt worry, we‚Äôll walk you through it. Once PsychoPy is up and running, adding DeToX is a breeze.",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "Vignettes/Installation.html#installing-psychopy",
    "href": "Vignettes/Installation.html#installing-psychopy",
    "title": "Installation",
    "section": "Installing PsychoPy",
    "text": "Installing PsychoPy\nSince PsychoPy is the main challenge, let‚Äôs tackle that first. You have two main options:\n\nPackage Installation\nInstall PsychoPy like any other Python package using pip. This approach is flexible and ideal if you prefer working in an IDE (like Positron, VS Code, PyCharm, or Spyder) where you have full control over your Python environment.\nStandalone Installation\nUse the PsychoPy standalone installer, which bundles PsychoPy and all its dependencies into a single, ready-to-use application. This is often the easiest way to get started, especially if you‚Äôre not familiar with managing Python environments or just want to hit the ground running.\n\nWe like installing psychopy as a package but you do you!\n\nPackageStandalone\n\n\nThis method is ideal if you prefer working in an IDE (like Positron, VS Code, PyCharm, or Spyder) and want full control over your Python environment.\n\nStep 1: Create a Virtual Environment\nWe like to use miniconda to handle our environments and Python installations. Any other method would work as well, but for simplicity we‚Äôll show you how we prefer to do it.\nWe recommend using Python 3.10 for the best compatibility:\nconda create -n detox_env python=3.10\nThis will create an environment called detox_env with python 3.10. Exactly what we need!\nYou will probably need to confirm by pressing y, and after a few seconds you‚Äôll have your environment with Python 3.10! Great!\n\n\nStep 2: Activate Environment and Install PsychoPy\nNow let‚Äôs activate this environment (making sure we‚Äôre using it) and then install PsychoPy:\nconda activate detox_env\npip install psychopy\nthis will take some time but if you are lucky you will have psychopy in your enviroment\nAgain, confirm if needed and you‚Äôre done! Amazing!\n\n\n\nPsychoPy is a large package with many dependencies, and sometimes (depending on your operating system) installing it can be quite tricky! For this reason, the PsychoPy website suggests using the standalone installation method. This is like installing regular software on your computer - it will install PsychoPy and all its dependencies in one go.\n\nStep 1: Install PsychoPy Standalone\n\nGo to the PsychoPy download page\nDownload the standalone installer for your operating system\nRun the installer and follow the setup instructions\n\nYou are done!!! Great!",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "Vignettes/Installation.html#installing-detox",
    "href": "Vignettes/Installation.html#installing-detox",
    "title": "Installation",
    "section": "Installing DeToX",
    "text": "Installing DeToX\nOnce PsychoPy is installed, we can look at DeToX. Let‚Äôs gets our hand dirty! The installation is the same for both the Package and Standalone PsychoPy installations but some steps differ.\n\nPackageStandalone\n\n\nAgain make sure to be in the correct environment if you installed PsychoPy as a package. with the following command:\nconda activate detox_env\nThen simply run:\npip install dvst-detox\nWait a few seconds, confirm if needed, and you are done!\n\n\n\nOpen PsychoPy\nGo to Coder View (the interface with the code editor)\nOpen the Tools menu\nSelect ‚ÄúPlugins/package manager‚Ä¶‚Äù\nClick on ‚ÄúPackages‚Äù in the top tabs\nClick the ‚ÄúOpen PIP terminal‚Äù button\nType the following command: pip install devst-detox\n\nThat‚Äôs it! You now have both PsychoPy and DeToX installed and ready to use.\n\n\n\n\n\n\n\n\n\nInstall from git\n\n\n\nIf you want to install the latest development version of DeToX directly from the GitHub repository, you can do so by replacing the installation command with the following:\npip install git+https://github.com/DevStart-Hub/DeToX.git\n\n\n\n\n\n\n\n\nImportant: DeToX requires coding\n\n\n\nDeToX is a code-based library that works with PsychoPy‚Äôs Coder interface. If you typically use PsychoPy‚Äôs Builder (the drag-and-drop visual interface), you‚Äôll need to switch to the Coder interface to use DeToX. Don‚Äôt worry - we provide plenty of code examples to get you started!",
    "crumbs": [
      "Installation"
    ]
  },
  {
    "objectID": "Vignettes/Calibration.html",
    "href": "Vignettes/Calibration.html",
    "title": "Calibration",
    "section": "",
    "text": "Good eye tracking data starts long before you present your first stimulus‚Äîit begins with proper setup and calibration. Even the most carefully designed experiment will produce noisy, unusable data if the eye tracker isn‚Äôt configured correctly. This is particularly critical when testing infants and children, where data quality can be challenging even under ideal conditions. In this tutorial, we‚Äôll walk through how to use DeToX to ensure the best possible data quality from the start.\nWe‚Äôll focus on two essential steps:\nGet these right, and everything else falls into place.",
    "crumbs": [
      "Calibration"
    ]
  },
  {
    "objectID": "Vignettes/Calibration.html#part-1-positioning-your-participant",
    "href": "Vignettes/Calibration.html#part-1-positioning-your-participant",
    "title": "Calibration",
    "section": "Part 1: Positioning Your Participant",
    "text": "Part 1: Positioning Your Participant\n\nUnderstanding the Track Box\nEvery eye tracker has a track box‚Äîan invisible 3D zone where it can accurately detect eyes. Step outside this zone, and tracking quality drops fast! While we try to seat everyone consistently, even small differences in posture or chair height can matter. The good news? DeToX makes checking position as easy as calling a single method.\n\n\n\n\n\n\nWarning\n\n\n\nThis tutorial assumes you‚Äôve completed Getting started. If not, start there first!!!\n\n\n\n\nSetup Code\nLet‚Äôs begin with our standard setup:\n\nfrom psychopy import visual, core\nfrom DeToX import ETracker\n\n\n## Creat the window\nwin = visual.Window(\n    size=[1920, 1080],  # Window dimensions in pixels\n    fullscr=True,       # Expand to fill the entire screen\n    units='pix'         # Use pixels as the measurement unit\n)\n\n\n## Connect to the eyetracker\nET_controller = ETracker(win)\n\nWe‚Äôre following the same steps from the previous tutorial. First, we import the libraries we need‚Äîpsychopy.visual for creating our display window and DeToX.ETracker for controlling the eye tracker.\nNext, we create a PsychoPy window where all our stimuli will appear.\nFinally, we connect to the Tobii eye tracker by creating an ETracker object. This automatically searches for connected devices, establishes communication, and links the tracker to our window. You‚Äôll see connection info printed to confirm everything‚Äôs working. Now the ET_controller object is ready to control all eye-tracking operations!\n\n\nChecking Position in Real-Time\nNow for the magic‚Äîvisualizing where your participant‚Äôs eyes are in the track box:\n\nEt_controller.show_status()\n\n\n\nWhat You‚Äôll See\nWhen you call show_status(), an animated video of animals appears to keep infants engaged. Overlaid on the video are two colored circles‚Äîüîµ light blue and üî¥ pink ‚Äîshowing where the tracker detects your participant‚Äôs eyes in real-time. A black rectangle marks the track box boundaries where tracking works best.\nBelow the rectangle, a green bar with a moving black line indicates distance from the screen. The line should be centered on the bar (about 65 cm distance).\n\n\n\n\n\n\n\n\n\n\n\nSimulation Mode\n\n\n\nIf you‚Äôre running with simulate=True, the positioning interface uses your mouse instead of real eye tracker data. Move the mouse around to see the eye circles follow, and use the scroll wheel to simulate moving closer to or further from the screen.\n\n\nCenter both colored circles within the black rectangle. If they‚Äôre drifting toward the edges, ask the participant to lean in the needed direction, or adjust the eye tracker mount if possible.\nFor distance, watch the black line on the green bar. Too far left means too close (move back), too far right means too far (move forward). Aim for the center.\nPress SPACE when positioning looks good‚Äîyou‚Äôre ready for calibration!\n\n\n\n\n\n\nAdvanced\n\n\n\n\n\nCustomizing the Positioning Display\nBy default, show_status() displays an animated video of animals to keep infants engaged while you adjust their position. However, you can customize this behavior using the video_help parameter:\nvideo_help=True: Uses the built-in instructional video included with DeToX (default). This is the easiest option and works well for most studies.\nvideo_help=False: Disables the video entirely, showing only the eye position circles and track box. Useful if you prefer a minimal display or if video playback causes performance issues.\nvideo_help=visual.MovieStim(...): Uses your own custom video. You‚Äôll need to pre-load and configure the MovieStim object yourself, including setting the appropriate size and position for your display layout.",
    "crumbs": [
      "Calibration"
    ]
  },
  {
    "objectID": "Vignettes/Calibration.html#part-2-calibration",
    "href": "Vignettes/Calibration.html#part-2-calibration",
    "title": "Calibration",
    "section": "Part 2: Calibration",
    "text": "Part 2: Calibration\nWith your participant positioned correctly, you‚Äôre ready for calibration. But what exactly is calibration, and why do we need it?\n\nUnderstanding Calibration\nCalibration teaches the eye tracker how to interpret each participant‚Äôs unique eye characteristics. Everyone‚Äôs eyes are slightly different‚Äîdifferent sizes, shapes, reflection patterns‚Äîso the tracker needs a personalized model to accurately estimate where someone is looking.\nThe process is straightforward: you present targets at known locations on the screen, the participant looks at each one, and the tracker records the relationship between eye features and screen coordinates. This creates a custom mapping for that individual.\n\n\n\nImage from tobii website\n\n\nSounds complex, right? It is‚Äîbut DeToX handles the complexity for you. Here‚Äôs how simple it becomes:\n\n\nLaunching Calibration with DeToX\n\nET_controller.calibrate(\n    calibration_points = 5,\n    shuffle=True,\n    audio=True,\n    anim_type='zoom',\n    visualization_style='circles'\n)\n\nLet‚Äôs break down what‚Äôs happening here:\ncalibration_points=5: Uses a standard 5-point calibration pattern (corners plus center). This is the default and works well for most studies. You can also choose 9 points for higher precision, or pass a custom list of coordinates in height units for specialized configurations.\nshuffle=True: Randomizes which image appears at each calibration point. This prevents habituation and keeps participants engaged throughout the procedure.\naudio=True: Plays an attention-getting sound along with the visual stimulus to help capture and maintain the participant‚Äôs focus.\nanim_type='zoom': Makes the stimuli gently pulse in size to attract attention. You can also use 'trill' for a rotating animation.\nvisualization_style='circles': Displays the calibration results using dots at each point. You can also choose 'lines' to show lines connecting the target to where the gaze landed.\n\n\n\n\n\n\nCustomize calibration\n\n\n\n\n\nYou can customize several aspects of the calibration procedure to match your experimental needs, let‚Äôs explore some of the key parameters:\ncalibration_points: By default uses a 5-point pattern (corners plus center). You can specify 9 for higher precision, or pass a custom list of coordinates in height units for specialized configurations tailored to your stimulus regions.\ninfant_stims: By default (infant_stims=True), DeToX uses a set of engaging animal cartoons included with the package. These work well for most infant studies, but you can provide a list of file paths to your own images if you prefer custom stimuli that better match your study‚Äôs theme or participant age group.\nstim_size: This parameter controls the size of the calibration stimuli. You can choose 'small' or 'big' to adjust how large the images appear on screen. We strongly recommend using 'big' for infant and child participants to make the targets more noticeable and easier to fixate on. On the other hand, 'small' may be suitable for adult participants to achiveive higher calibration precision.\naudio: By default (audio=True), an attention-getting sound included with DeToX plays automatically when stimuli appear. Set this to None to disable audio entirely, or pass your own pre-loaded psychopy.sound.Sound object to use custom sounds that fit your experimental context.\nRemeber that while you can customize these parameters you can also use default settings to make the calibration process quick and easy. You could just call\nET_controller.calibrate()\nto run a standard 5-point calibration with default settings.\n\n\n\n\n\nThe Calibration Process\n\nStep 1: Instructions\nFirst, you‚Äôll see instructions explaining the controls:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Calibration Setup ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇMouse-Based Calibration Setup:                              ‚îÇ\n‚îÇ                                                            ‚îÇ\n‚îÇ    - Press number keys (1-5) to select calibration points  ‚îÇ\n‚îÇ    - Move your mouse to the animated stimulus              ‚îÇ\n‚îÇ    - Press SPACE to collect samples at the selected point  ‚îÇ\n‚îÇ    - Press ENTER to finish collecting and see results      ‚îÇ\n‚îÇ    - Press ESCAPE to exit calibration                      ‚îÇ\n‚îÇ                                                            ‚îÇ\n‚îÇ    Any key will start calibration immediately!             ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nthese instructions will tell you how to control the calibration using your keyboard.\nPress any key when you‚Äôre ready to begin.\n\n\nData Collection\nThe calibration screen appears with a thin red border indicating you‚Äôre in calibration mode. Press a number key (1-5) to display an animated stimulus at the corresponding calibration point. The participant should look at the stimulus while it animates. When you‚Äôre confident they‚Äôre fixating on the target, press SPACE to collect gaze samples. The system waits briefly (0.25 seconds) to ensure stable fixation, then records the eye tracking data for that point. Repeat this process for all calibration points. You don‚Äôt need to go in order.\n\n\nStep 3: Review Results\nOnce you‚Äôve collected data for all points (or whenever you‚Äôre satisfied), press ENTER to compute and visualize the calibration results. You‚Äôll see a display showing the calibration targets and dots (or lines if you selected them in the visualization_style) to where the gaze samples actually landed.\nAt this stage, you have several options:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Calibration Results ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇReview calibration results above.                 ‚îÇ\n‚îÇ                                                  ‚îÇ\n‚îÇ    - Press ENTER to accept calibration           ‚îÇ\n‚îÇ    - Press Numbers ‚Üí SPACE to retry some points  ‚îÇ\n‚îÇ    - Press ESCAPE to restart calibration         ‚îÇ\n‚îÇ                                                  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nAccept the calibration: If the results look good across all points, press SPACE to accept and move forward with your experiment.\nRetry specific points: Notice one or two points with poor accuracy? Press the number keys corresponding to those points‚Äîthey‚Äôll highlight in yellow to confirm your selection. You can select multiple points if needed. Once you‚Äôve marked all points for retry, press SPACE again to recollect data for just those points. This targeted approach is especially valuable with infants and children, where you might get excellent data at most points but struggle with one or two locations.\nStart over: If the overall quality is poor or you want a fresh start, press ESCAPE to discard all data and restart the entire calibration procedure from the beginning.\n\n\n\n\n\n\nSimulation Mode\n\n\n\nWhen running with simulate=True, the calibration procedure uses mouse position instead of real eye tracker data. Press number keys to display stimuli at calibration points, move your mouse to each target location, and press SPACE to ‚Äúcollect‚Äù samples. This allows you to test your entire experiment workflow without needing physical eye tracking hardware.\n\n\n\n\n\nCalibration Complete!\nCongratulations! You‚Äôve successfully positioned your participant and completed the calibration procedure using DeToX. With accurate calibration in place, you‚Äôre now ready to present your experimental stimuli and collect high-quality eye tracking data.\nHere‚Äôs a video demonstrating the entire calibration workflow from start to finish:",
    "crumbs": [
      "Calibration"
    ]
  },
  {
    "objectID": "Vignettes/Calibration.html#save-and-load-calibration",
    "href": "Vignettes/Calibration.html#save-and-load-calibration",
    "title": "Calibration",
    "section": "Save and Load calibration",
    "text": "Save and Load calibration\nWhile we recommend performing calibration at the start of each session to ensure optimal accuracy, DeToX also allows you to save and load calibration data for convenience. In our opinion, this should only be used in special circumstances where there is a headrest and little to no chance of movement between sessions. However, if you need to save and reuse calibration data, here‚Äôs how:\n\nSaving Calibration\nAfter completing a successful calibration, you can save the calibration data to a file:\n# Save with custom filename\nET_controller.save_calibration(filename=\"S01_calibration.dat\")\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Calibration Saved ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇCalibration data saved to:    ‚îÇ\n‚îÇS01_calibration.dat           ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nThe calibration data is saved as a binary file (.dat format) that can be reloaded in future sessions. If you don‚Äôt specify a filename, DeToX automatically generates a timestamped name like 2024-01-15_14-30-00_calibration.dat.\nyou can also choose to use a GUI file dialog to select the save location:\n# Save with GUI file dialog\nET.save_calibration(use_gui=True)\n\n\nLoading Calibration\nTo reuse a previously saved calibration in a new session:\n# Load from specific file\nET.load_calibration(filename=\"S01_calibration.dat\")\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Calibration Loaded ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇCalibration data loaded from:  ‚îÇ\n‚îÇS01_calibration.dat            ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\nor again, use a GUI file dialog to select the file:\n# Load with GUI file dialog\nET.load_calibration(use_gui=True)",
    "crumbs": [
      "Calibration"
    ]
  }
]